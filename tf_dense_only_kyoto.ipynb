{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:09:04.359633Z",
     "start_time": "2017-07-21T21:09:03.922967Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:09:04.370612Z",
     "start_time": "2017-07-21T21:09:04.361161Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:09:04.470771Z",
     "start_time": "2017-07-21T21:09:04.372308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20140115': {'x': 'dataset/Kyoto2016/2014/01/20140115_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140115_y.csv'}, '20140107': {'y': 'dataset/Kyoto2016/2014/01/20140107_y.csv', 'x': 'dataset/Kyoto2016/2014/01/20140107_x.csv'}, '20140109': {'x': 'dataset/Kyoto2016/2014/01/20140109_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140109_y.csv'}, '20140104': {'y': 'dataset/Kyoto2016/2014/01/20140104_y.csv', 'x': 'dataset/Kyoto2016/2014/01/20140104_x.csv'}, '20140119': {'x': 'dataset/Kyoto2016/2014/01/20140119_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140119_y.csv'}, '20140120': {'x': 'dataset/Kyoto2016/2014/01/20140120_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140120_y.csv'}, '20140123': {'x': 'dataset/Kyoto2016/2014/01/20140123_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140123_y.csv'}, '20140118': {'x': 'dataset/Kyoto2016/2014/01/20140118_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140118_y.csv'}, '20140108': {'x': 'dataset/Kyoto2016/2014/01/20140108_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140108_y.csv'}, '20140112': {'y': 'dataset/Kyoto2016/2014/01/20140112_y.csv', 'x': 'dataset/Kyoto2016/2014/01/20140112_x.csv'}, '20140126': {'y': 'dataset/Kyoto2016/2014/01/20140126_y.csv', 'x': 'dataset/Kyoto2016/2014/01/20140126_x.csv'}, '20140129': {'x': 'dataset/Kyoto2016/2014/01/20140129_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140129_y.csv'}, '20140130': {'x': 'dataset/Kyoto2016/2014/01/20140130_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140130_y.csv'}, '20140128': {'x': 'dataset/Kyoto2016/2014/01/20140128_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140128_y.csv'}}\n",
      "----------------------------------------------------------------------------------------\n",
      "{'20151224': {'y': 'dataset/Kyoto2016/2015/12/20151224_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151224_x.csv'}, '20151204': {'y': 'dataset/Kyoto2016/2015/12/20151204_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151204_x.csv'}, '20151216': {'x': 'dataset/Kyoto2016/2015/12/20151216_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151216_y.csv'}, '20151222': {'x': 'dataset/Kyoto2016/2015/12/20151222_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151222_y.csv'}, '20151214': {'x': 'dataset/Kyoto2016/2015/12/20151214_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151214_y.csv'}, '20151202': {'x': 'dataset/Kyoto2016/2015/12/20151202_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151202_y.csv'}, '20151227': {'y': 'dataset/Kyoto2016/2015/12/20151227_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151227_x.csv'}, '20151203': {'y': 'dataset/Kyoto2016/2015/12/20151203_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151203_x.csv'}, '20151223': {'y': 'dataset/Kyoto2016/2015/12/20151223_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151223_x.csv'}, '20151205': {'x': 'dataset/Kyoto2016/2015/12/20151205_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151205_y.csv'}, '20151229': {'x': 'dataset/Kyoto2016/2015/12/20151229_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151229_y.csv'}, '20151208': {'x': 'dataset/Kyoto2016/2015/12/20151208_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151208_y.csv'}, '20151219': {'y': 'dataset/Kyoto2016/2015/12/20151219_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151219_x.csv'}, '20151206': {'y': 'dataset/Kyoto2016/2015/12/20151206_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151206_x.csv'}, '20151225': {'x': 'dataset/Kyoto2016/2015/12/20151225_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151225_y.csv'}, '20151210': {'x': 'dataset/Kyoto2016/2015/12/20151210_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151210_y.csv'}, '20151217': {'x': 'dataset/Kyoto2016/2015/12/20151217_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151217_y.csv'}, '20151207': {'x': 'dataset/Kyoto2016/2015/12/20151207_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151207_y.csv'}, '20151215': {'x': 'dataset/Kyoto2016/2015/12/20151215_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151215_y.csv'}, '20151213': {'y': 'dataset/Kyoto2016/2015/12/20151213_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151213_x.csv'}, '20151209': {'x': 'dataset/Kyoto2016/2015/12/20151209_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151209_y.csv'}, '20151228': {'y': 'dataset/Kyoto2016/2015/12/20151228_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151228_x.csv'}, '20151226': {'y': 'dataset/Kyoto2016/2015/12/20151226_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151226_x.csv'}, '20151218': {'x': 'dataset/Kyoto2016/2015/12/20151218_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151218_y.csv'}, '20151231': {'x': 'dataset/Kyoto2016/2015/12/20151231_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151231_y.csv'}, '20151212': {'x': 'dataset/Kyoto2016/2015/12/20151212_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151212_y.csv'}, '20151211': {'x': 'dataset/Kyoto2016/2015/12/20151211_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151211_y.csv'}, '20151221': {'y': 'dataset/Kyoto2016/2015/12/20151221_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151221_x.csv'}, '20151201': {'x': 'dataset/Kyoto2016/2015/12/20151201_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151201_y.csv'}, '20151220': {'x': 'dataset/Kyoto2016/2015/12/20151220_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151220_y.csv'}, '20151230': {'y': 'dataset/Kyoto2016/2015/12/20151230_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151230_x.csv'}}\n"
     ]
    }
   ],
   "source": [
    "class preprocess:\n",
    "    \n",
    "    paths = {}\n",
    "\n",
    "    def get_files(folder_path):\n",
    "        paths = {}\n",
    "        for path, subdirs, files in os.walk(folder_path):\n",
    "            for name in files:\n",
    "                if name.endswith(\"csv\"):\n",
    "                    key = name.split(\"_\")[0]\n",
    "\n",
    "                    if paths.get(key) is None:\n",
    "                        paths[key] = {}\n",
    "\n",
    "                    if name.endswith(\"_x.csv\"):\n",
    "                        x = os.path.join(path, name)\n",
    "                        paths[key]['x'] = x\n",
    "                    elif name.endswith(\"_y.csv\"):\n",
    "                        y = os.path.join(path, name)\n",
    "                        paths[key]['y'] = y\n",
    "        preprocess.paths = paths\n",
    "        return paths\n",
    "\n",
    "    def get_data(paths):\n",
    "        for key, value in paths.items():\n",
    "            x = pd.read_csv(value['x'])\n",
    "            y = pd.read_csv(value['y'])\n",
    "            #print(x.shape)\n",
    "            #print(x.values.shape)\n",
    "            #print(y.sum())\n",
    "            yield key, x.values, y.values\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "train_paths = preprocess.get_files(\"dataset/Kyoto2016/2014/01\")\n",
    "test_paths = preprocess.get_files(\"dataset/Kyoto2016/2015/12\")\n",
    "#test_paths.update(preprocess.get_files(\"dataset/Kyoto2016/2015/11\"))\n",
    "#test_paths.update(preprocess.get_files(\"dataset/Kyoto2016/2015/10\"))\n",
    "\n",
    "\n",
    "paths = {}\n",
    "keys = train_paths.keys()\n",
    "for key in list(keys)[0:7]:\n",
    "    paths.update({key: train_paths[key]})\n",
    "train_paths = paths\n",
    "\n",
    "print(train_paths)\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "#test_paths = test_paths.popitem()\n",
    "#test_paths = {test_paths[0]: test_paths[1]}\n",
    "print(test_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:09:05.559526Z",
     "start_time": "2017-07-21T21:09:04.472187Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:09:05.753848Z",
     "start_time": "2017-07-21T21:09:05.561138Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 42\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 42\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 18\n",
    "\n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "            #hidden_encoder = tf.layers.dense(self.x, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            #hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(hidden_encoder, classes, activation=tf.nn.softmax)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "            #loss = tf.losses.mean_squared_error(labels = self.y_, predictions = self.y)\n",
    "            #loss = tf.clip_by_value(loss, -1e-1, 1e-1)\n",
    "            #loss = tf.where(tf.is_nan(loss), 1e-1, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-1), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-1), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = loss\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "            \n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:09:06.327516Z",
     "start_time": "2017-07-21T21:09:05.755423Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import metrics as me\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['key', 'no_of_features','hidden_layers','train_score', 'test_score', 'quality_score', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "    results = []\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "    \n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 1000\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_dense_only_nsl_kdd/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            for c, lr in enumerate(lrs):\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    \n",
    "                    for key, x_train, y_train in preprocess.get_data(train_paths):\n",
    "                        x_train, x_valid, y_train, y_valid, = ms.train_test_split(x_train, \n",
    "                                                                                  y_train, \n",
    "                                                                                  test_size=0.1)\n",
    "                        batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                                   batch_iterations)\n",
    "\n",
    "                        for i in batch_indices:\n",
    "\n",
    "                            def train_batch():\n",
    "                                nonlocal train_loss\n",
    "                                _, train_loss = sess.run([net.train_op, \n",
    "                                                                   net.regularized_loss, \n",
    "                                                                   ], #net.summary_op\n",
    "                                                                  feed_dict={net.x: x_train[i,:], \n",
    "                                                                             net.y_: y_train[i,:], \n",
    "                                                                             net.keep_prob:0.5, net.lr:lr})\n",
    "\n",
    "                            train_batch()\n",
    "                            #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                            #print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                            while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1):\n",
    "                                print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                                net.saver.restore(sess, \n",
    "                                                  tf.train.latest_checkpoint('dataset/tf_dense_only_nsl_kdd/hidden_layers_{}_features_count_{}'\n",
    "                                                                             .format(epochs,h,f)))\n",
    "                                train_batch()\n",
    "\n",
    "\n",
    "                        valid_accuracy = sess.run(net.regularized_loss, #net.summary_op \n",
    "                                                              feed_dict={net.x: x_valid, \n",
    "                                                                         net.y_: y_valid, \n",
    "                                                                         net.keep_prob:1, net.lr:lr})\n",
    "                        #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "                    \n",
    "                        print(\"Key {} | Training Loss: {:.6f} | Validation Loss: {:.6f}\".format(key, train_loss, valid_accuracy))\n",
    "                    \n",
    "                    end_time = time.perf_counter() \n",
    "                    for key, x_test, y_test in preprocess.get_data(test_paths):\n",
    "                        accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                                               net.pred, \n",
    "                                                                               net.actual, net.y], \n",
    "                                                                              feed_dict={net.x: x_test, \n",
    "                                                                                         net.y_: y_test, \n",
    "                                                                                         net.keep_prob:1, net.lr:lr})\n",
    "                        \n",
    "                        \n",
    "                        q_score = me.matthews_corrcoef(actual_value, pred_value)\n",
    "                        recall = me.recall_score(actual_value, pred_value)\n",
    "                        prec = me.precision_score(actual_value, pred_value)\n",
    "                        accuracy = me.roc_auc_score(actual_value, pred_value)\n",
    "\n",
    "                        print(\"Key {} Test Accuracy: {} Quality score: {}, recall {}, precision {}\".format(key, accuracy, q_score, recall, prec))\n",
    "\n",
    "                        if accuracy > Train.best_acc_global:\n",
    "                            Train.best_acc_global = accuracy\n",
    "                            Train.pred_value = pred_value\n",
    "                            Train.actual_value = actual_value\n",
    "\n",
    "                            Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value, \"Actual\": actual_value})\n",
    "                        Train.predictions.update({\"{}_{}_{}\".format(key,f,h):(curr_pred, \n",
    "                                                   Train.result(key, f, h, valid_accuracy, accuracy, q_score, end_time - start_time))})\n",
    "\n",
    "                            #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:09:06.394780Z",
     "start_time": "2017-07-21T21:09:06.329184Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "df_results = []\n",
    "past_scores = []\n",
    "\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "    def start_training():\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        print(\"********************************** Training ******************************\")\n",
    "        Train.predictions = {}\n",
    "        Train.results = []\n",
    "    \n",
    "        \n",
    "        features_arr = [1, 4, 8, 16, 42]\n",
    "        hidden_layers_arr = [1, 3]\n",
    "\n",
    "        epochs = [1]\n",
    "        lrs = [1e-5]\n",
    "        print(\"***************************** Entering Loop **********************\")\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - hidden layers:{} features count:{}\".format(h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "            \n",
    "        dict1 = {}\n",
    "        dict2 = []\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "        Train.predictions = dict1\n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "\n",
    "        #temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        if not os.path.isfile('dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl'):\n",
    "            past_scores = df_results#temp\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl\")\n",
    "\n",
    "        past_scores.append(df_results, ignore_index=True).to_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:31:24.685127Z",
     "start_time": "2017-07-21T21:09:06.396325Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************** Training ******************************\n",
      "***************************** Entering Loop **********************\n",
      "Current Layer Attributes - hidden layers:1 features count:1\n",
      "Key 20140115 | Training Loss: 0.530602 | Validation Loss: 0.503857\n",
      "Key 20140107 | Training Loss: 0.496137 | Validation Loss: 0.469969\n",
      "Key 20140109 | Training Loss: 0.430401 | Validation Loss: 0.413722\n",
      "Key 20140104 | Training Loss: 0.418857 | Validation Loss: 0.401311\n",
      "Key 20140119 | Training Loss: 0.399089 | Validation Loss: 0.377746\n",
      "Key 20140120 | Training Loss: 0.370452 | Validation Loss: 0.357932\n",
      "Key 20140123 | Training Loss: 0.360356 | Validation Loss: 0.342820\n",
      "Key 20140118 | Training Loss: 0.347077 | Validation Loss: 0.333085\n",
      "Key 20140108 | Training Loss: 0.339700 | Validation Loss: 0.331978\n",
      "Key 20140112 | Training Loss: 0.340953 | Validation Loss: 0.324984\n",
      "Key 20140126 | Training Loss: 0.325432 | Validation Loss: 0.326469\n",
      "Key 20140129 | Training Loss: 0.324306 | Validation Loss: 0.320781\n",
      "Key 20140130 | Training Loss: 0.328479 | Validation Loss: 0.321698\n",
      "Key 20140128 | Training Loss: 0.322108 | Validation Loss: 0.320999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:516: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(var_yt * var_yp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151224 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9828865688487585\n",
      "Key 20151204 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867503661554218\n",
      "Key 20151216 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9788375571943093\n",
      "Key 20151222 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867417614464858\n",
      "Key 20151214 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9849720497672902\n",
      "Key 20151202 Test Accuracy: 0.4999978499617293 Quality score: -0.0004470662704783412, recall 0.9999956999234586, precision 0.9535198389431258\n",
      "Key 20151227 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9881973019016337\n",
      "Key 20151203 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9062532636026961\n",
      "Key 20151223 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9652120631124332\n",
      "Key 20151205 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.986512289703779\n",
      "Key 20151229 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9727608594282435\n",
      "Key 20151208 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9833508070338446\n",
      "Key 20151219 Test Accuracy: 0.4999966647767068 Quality score: -0.0014406453955868126, recall 0.9999933295534136, precision 0.6888575406544225\n",
      "Key 20151206 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9852853269571529\n",
      "Key 20151225 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9858456179148585\n",
      "Key 20151210 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9866921178466759\n",
      "Key 20151217 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.966389328069453\n",
      "Key 20151207 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9642177070037038\n",
      "Key 20151215 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9808085269898045\n",
      "Key 20151213 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9905969009923565\n",
      "Key 20151209 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9822953999241051\n",
      "Key 20151228 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.988685517789228\n",
      "Key 20151226 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9024700130748678\n",
      "Key 20151218 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9788305128278303\n",
      "Key 20151231 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.925385352090802\n",
      "Key 20151212 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9781912431731926\n",
      "Key 20151211 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9899781954842789\n",
      "Key 20151221 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867129288367005\n",
      "Key 20151201 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9695523681747558\n",
      "Key 20151220 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.938288391111464\n",
      "Key 20151230 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9489095185259758\n",
      "Current Layer Attributes - hidden layers:1 features count:4\n",
      "Key 20140115 | Training Loss: 0.830686 | Validation Loss: 0.839280\n",
      "Key 20140107 | Training Loss: 0.750709 | Validation Loss: 0.716252\n",
      "Key 20140109 | Training Loss: 0.636779 | Validation Loss: 0.651641\n",
      "Key 20140104 | Training Loss: 0.557084 | Validation Loss: 0.569394\n",
      "Key 20140119 | Training Loss: 0.520342 | Validation Loss: 0.514490\n",
      "Key 20140120 | Training Loss: 0.496902 | Validation Loss: 0.462575\n",
      "Key 20140123 | Training Loss: 0.447201 | Validation Loss: 0.415567\n",
      "Key 20140118 | Training Loss: 0.407336 | Validation Loss: 0.380600\n",
      "Key 20140108 | Training Loss: 0.376497 | Validation Loss: 0.370343\n",
      "Key 20140112 | Training Loss: 0.372242 | Validation Loss: 0.349578\n",
      "Key 20140126 | Training Loss: 0.369902 | Validation Loss: 0.343764\n",
      "Key 20140129 | Training Loss: 0.339973 | Validation Loss: 0.329766\n",
      "Key 20140130 | Training Loss: 0.345258 | Validation Loss: 0.331672\n",
      "Key 20140128 | Training Loss: 0.339684 | Validation Loss: 0.328882\n",
      "Key 20151224 Test Accuracy: 0.49999187040656523 Quality score: -5.172514590177577e-05, recall 0.9983349361964775, precision 0.9828862949040446\n",
      "Key 20151204 Test Accuracy: 0.5009014423183022 Quality score: 0.010533283624255367, recall 0.9996407224744424, precision 0.9867739462503493\n",
      "Key 20151216 Test Accuracy: 0.4997428673447105 Quality score: -0.002777429613218054, recall 0.9992784378403331, precision 0.978826896821181\n",
      "Key 20151222 Test Accuracy: 0.5000488113414213 Quality score: 0.0008355354768326384, recall 0.9998226707994222, precision 0.98674303881927\n",
      "Key 20151214 Test Accuracy: 0.5004478566867658 Quality score: 0.007189849323973614, recall 0.9997836755642462, precision 0.9849853112633149\n",
      "Key 20151202 Test Accuracy: 0.4998506667931745 Quality score: -0.002683169793721313, recall 0.9994366899730815, precision 0.9535067854739986\n",
      "Key 20151227 Test Accuracy: 0.49980800537584946 Quality score: -0.0021292791185544033, recall 0.9996160107516989, precision 0.9881928215834905\n",
      "Key 20151203 Test Accuracy: 0.5130209832707341 Quality score: 0.14274273214963729, recall 0.9996055296761355, precision 0.9084720364790616\n",
      "Key 20151223 Test Accuracy: 0.49962306057111566 Quality score: -0.004686319182679525, recall 0.9991040756876859, precision 0.965186727532605\n",
      "Key 20151205 Test Accuracy: 0.5003899259106457 Quality score: 0.003682279487106755, recall 0.9994133582438113, precision 0.986522672450041\n",
      "Key 20151229 Test Accuracy: 0.49998709420492926 Quality score: -0.0008385124841724435, recall 0.9999741884098585, precision 0.9727601754769839\n",
      "Key 20151208 Test Accuracy: 0.49979929754743435 Quality score: -0.0015161694681396865, recall 0.9988444472818974, precision 0.9833442276749799\n",
      "Key 20151219 Test Accuracy: 0.4999966647767068 Quality score: -0.0014406453955868113, recall 0.9999933295534136, precision 0.6888575406544225\n",
      "Key 20151206 Test Accuracy: 0.4997397681351925 Quality score: -0.0017820295975401414, recall 0.9987541100824996, precision 0.9852777718401984\n",
      "Key 20151225 Test Accuracy: 0.4997302651109212 Quality score: -0.00182323040824917, recall 0.9987692063366023, precision 0.9858380809154547\n",
      "Key 20151210 Test Accuracy: 0.500258505081448 Quality score: 0.007111172437126925, recall 0.9999374680012038, precision 0.9866989070658923\n",
      "Key 20151217 Test Accuracy: 0.4993218409497479 Quality score: -0.00675622624914131, recall 0.9986436818994958, precision 0.9663452156889899\n",
      "Key 20151207 Test Accuracy: 0.499812244011651 Quality score: -0.0022678055592812727, recall 0.9990396927016645, precision 0.9642047388401623\n",
      "Key 20151215 Test Accuracy: 0.5001966693240593 Quality score: 0.002834066597757946, recall 0.9996448356541066, precision 0.9808159335527167\n",
      "Key 20151213 Test Accuracy: 0.5001217792761452 Quality score: 0.004977188840157099, recall 0.9999799844879782, precision 0.9905991697131173\n",
      "Key 20151209 Test Accuracy: 0.49992059097484237 Quality score: -0.0016769760863012666, recall 0.9998411819496847, precision 0.9822926374650512\n",
      "Key 20151228 Test Accuracy: 0.49999174515647055 Quality score: -0.00043220551786734196, recall 0.9999834903129411, precision 0.9886853331011818\n",
      "Key 20151226 Test Accuracy: 0.4999937009262788 Quality score: -0.0011084724286647934, recall 0.9999874018525576, precision 0.9024689041999227\n",
      "Key 20151218 Test Accuracy: 0.500255730162331 Quality score: 0.004194345229019081, recall 0.9997026169706582, precision 0.9788411142385713\n",
      "Key 20151231 Test Accuracy: 0.4999895107462405 Quality score: -0.0012511332224620759, recall 0.999979021492481, precision 0.9253839035533323\n",
      "Key 20151212 Test Accuracy: 0.4999848590949524 Quality score: -0.0008126673338152936, recall 0.9999697181899048, precision 0.9781905971481151\n",
      "Key 20151211 Test Accuracy: 0.4999910466469693 Quality score: -0.0004236280078232196, recall 0.9999820932939386, precision 0.9899780178221098\n",
      "Key 20151221 Test Accuracy: 0.4998822849877571 Quality score: -0.0008801151108907787, recall 0.9990578561945955, precision 0.9867098393244012\n",
      "Key 20151201 Test Accuracy: 0.49954549908176193 Quality score: -0.004862440430224395, recall 0.9989395747837541, precision 0.9695255061775118\n",
      "Key 20151220 Test Accuracy: 0.4999735564464095 Quality score: -0.0018066295801295847, recall 0.999947112892819, precision 0.9382853286221887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151230 Test Accuracy: 0.49952596578827374 Quality score: -0.006962821780055733, recall 0.9990519315765475, precision 0.9488635145507013\n",
      "Current Layer Attributes - hidden layers:1 features count:8\n",
      "Key 20140115 | Training Loss: 0.745919 | Validation Loss: 0.755287\n",
      "Key 20140107 | Training Loss: 0.661609 | Validation Loss: 0.662953\n",
      "Key 20140109 | Training Loss: 0.615492 | Validation Loss: 0.567891\n",
      "Key 20140104 | Training Loss: 0.534380 | Validation Loss: 0.517391\n",
      "Key 20140119 | Training Loss: 0.480624 | Validation Loss: 0.464661\n",
      "Key 20140120 | Training Loss: 0.444013 | Validation Loss: 0.422509\n",
      "Key 20140123 | Training Loss: 0.422808 | Validation Loss: 0.392726\n",
      "Key 20140118 | Training Loss: 0.410150 | Validation Loss: 0.368815\n",
      "Key 20140108 | Training Loss: 0.386851 | Validation Loss: 0.360383\n",
      "Key 20140112 | Training Loss: 0.370040 | Validation Loss: 0.344765\n",
      "Key 20140126 | Training Loss: 0.355536 | Validation Loss: 0.339238\n",
      "Key 20140129 | Training Loss: 0.346548 | Validation Loss: 0.327575\n",
      "Key 20140130 | Training Loss: 0.344980 | Validation Loss: 0.329618\n",
      "Key 20140128 | Training Loss: 0.330111 | Validation Loss: 0.327700\n",
      "Key 20151224 Test Accuracy: 0.49706699967942736 Quality score: -0.01004836053677347, recall 0.9941339993588547, precision 0.9827873272347148\n",
      "Key 20151204 Test Accuracy: 0.49959688826233556 Quality score: -0.001877299562796951, recall 0.9975721549030495, precision 0.9867397999842055\n",
      "Key 20151216 Test Accuracy: 0.4984022552178804 Quality score: -0.008236294598869077, recall 0.9968045104357608, precision 0.9787711562326723\n",
      "Key 20151222 Test Accuracy: 0.4974841419668025 Quality score: -0.008188074774804462, recall 0.994968283933605, precision 0.9866756057708512\n",
      "Key 20151214 Test Accuracy: 0.49760134375649506 Quality score: -0.008510937050766014, recall 0.9952026875129901, precision 0.9849007022831548\n",
      "Key 20151202 Test Accuracy: 0.49901313243375733 Quality score: -0.009587081357099481, recall 0.9980262648675147, precision 0.953432389465598\n",
      "Key 20151227 Test Accuracy: 0.49826204866263746 Quality score: -0.0064161071955286435, recall 0.9965240973252749, precision 0.9881566213450524\n",
      "Key 20151203 Test Accuracy: 0.4986991343814129 Quality score: -0.015635864373848056, recall 0.9973982687628258, precision 0.906031702573972\n",
      "Key 20151223 Test Accuracy: 0.49786448913523057 Quality score: -0.012010251033894448, recall 0.9955869328159157, precision 0.9650680376762975\n",
      "Key 20151205 Test Accuracy: 0.49876025166969434 Quality score: -0.0050129049979415, recall 0.9967006071929005, precision 0.9864791899348367\n",
      "Key 20151229 Test Accuracy: 0.4970685408339356 Quality score: -0.012673471886644389, recall 0.9941370816678712, precision 0.9726046176046176\n",
      "Key 20151208 Test Accuracy: 0.4958725699875507 Quality score: -0.011771215738797344, recall 0.9917451399751014, precision 0.9832145524513589\n",
      "Key 20151219 Test Accuracy: 0.4980922522762899 Quality score: -0.034500516741903076, recall 0.9961845045525798, precision 0.6880390314020345\n",
      "Key 20151206 Test Accuracy: 0.4980174099573689 Quality score: -0.007653441838140609, recall 0.9960348199147377, precision 0.985227613699687\n",
      "Key 20151225 Test Accuracy: 0.4988237979910271 Quality score: -0.005777042815388976, recall 0.9976475959820542, precision 0.9858127160826815\n",
      "Key 20151210 Test Accuracy: 0.4978231047919052 Quality score: -0.0076282176200634325, recall 0.9956462095838104, precision 0.986634702524341\n",
      "Key 20151217 Test Accuracy: 0.499087656529517 Quality score: -0.007838194312781573, recall 0.998175313059034, precision 0.9663299557279694\n",
      "Key 20151207 Test Accuracy: 0.498187894702576 Quality score: -0.011407775153213973, recall 0.996375789405152, precision 0.9640922262819706\n",
      "Key 20151215 Test Accuracy: 0.49715502374475035 Quality score: -0.01047909349324915, recall 0.9943100474895007, precision 0.980700823031892\n",
      "Key 20151213 Test Accuracy: 0.4987507167026507 Quality score: -0.0046142640293374395, recall 0.9972378593409893, precision 0.9905735637297175\n",
      "Key 20151209 Test Accuracy: 0.49818217718085084 Quality score: -0.008037309917602299, recall 0.9963643543617017, precision 0.9822319452613584\n",
      "Key 20151228 Test Accuracy: 0.4979693084917575 Quality score: -0.00679247441341876, recall 0.995938616983515, precision 0.9886399021048549\n",
      "Key 20151226 Test Accuracy: 0.4988193224980601 Quality score: -0.015089324922311917, recall 0.997609501522801, precision 0.902261721643024\n",
      "Key 20151218 Test Accuracy: 0.4968833092317022 Quality score: -0.011522480708229779, recall 0.9937666184634044, precision 0.9787005558873524\n",
      "Key 20151231 Test Accuracy: 0.4982989926819973 Quality score: -0.01595749920921486, recall 0.9965979853639946, precision 0.9251497103166231\n",
      "Key 20151212 Test Accuracy: 0.4990004223194292 Quality score: -0.00634047996147364, recall 0.9978310653519314, precision 0.9781485040871833\n",
      "Key 20151211 Test Accuracy: 0.4987465305757006 Quality score: -0.005018621269777643, recall 0.9974930611514012, precision 0.9899532613396046\n",
      "Key 20151221 Test Accuracy: 0.49811095408714356 Quality score: -0.0070984233839225626, recall 0.9962219081742871, precision 0.9866632107259832\n",
      "Key 20151201 Test Accuracy: 0.49792432463277425 Quality score: -0.011265416272623582, recall 0.9958486492655485, precision 0.9694293226678518\n",
      "Key 20151220 Test Accuracy: 0.4992252038797982 Quality score: -0.009786065204724897, recall 0.9984504077595964, precision 0.9381985339793764\n",
      "Key 20151230 Test Accuracy: 0.49725511618352797 Quality score: -0.016791176203391347, recall 0.9945102323670559, precision 0.9486419795618678\n",
      "Current Layer Attributes - hidden layers:1 features count:16\n",
      "Key 20140115 | Training Loss: 0.581885 | Validation Loss: 0.570870\n",
      "Key 20140107 | Training Loss: 0.495940 | Validation Loss: 0.483725\n",
      "Key 20140109 | Training Loss: 0.455782 | Validation Loss: 0.421262\n",
      "Key 20140104 | Training Loss: 0.410575 | Validation Loss: 0.391036\n",
      "Key 20140119 | Training Loss: 0.395265 | Validation Loss: 0.369897\n",
      "Key 20140120 | Training Loss: 0.373876 | Validation Loss: 0.352306\n",
      "Key 20140123 | Training Loss: 0.360100 | Validation Loss: 0.338508\n",
      "Key 20140118 | Training Loss: 0.347253 | Validation Loss: 0.332513\n",
      "Key 20140108 | Training Loss: 0.339629 | Validation Loss: 0.328972\n",
      "Key 20140112 | Training Loss: 0.336852 | Validation Loss: 0.325032\n",
      "Key 20140126 | Training Loss: 0.338205 | Validation Loss: 0.325298\n",
      "Key 20140129 | Training Loss: 0.323628 | Validation Loss: 0.319935\n",
      "Key 20140130 | Training Loss: 0.322290 | Validation Loss: 0.322138\n",
      "Key 20140128 | Training Loss: 0.327975 | Validation Loss: 0.321321\n",
      "Key 20151224 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9828865688487585\n",
      "Key 20151204 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867503661554218\n",
      "Key 20151216 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9788375571943093\n",
      "Key 20151222 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867417614464858\n",
      "Key 20151214 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9849720497672902\n",
      "Key 20151202 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9535200295215056\n",
      "Key 20151227 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9881973019016337\n",
      "Key 20151203 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9062532636026961\n",
      "Key 20151223 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9652120631124332\n",
      "Key 20151205 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.986512289703779\n",
      "Key 20151229 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9727608594282435\n",
      "Key 20151208 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9833508070338446\n",
      "Key 20151219 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.6888589703530795\n",
      "Key 20151206 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9852853269571529\n",
      "Key 20151225 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9858456179148585\n",
      "Key 20151210 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9866921178466759\n",
      "Key 20151217 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.966389328069453\n",
      "Key 20151207 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9642177070037038\n",
      "Key 20151215 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9808085269898045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151213 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9905969009923565\n",
      "Key 20151209 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9822953999241051\n",
      "Key 20151228 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.988685517789228\n",
      "Key 20151226 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9024700130748678\n",
      "Key 20151218 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9788305128278303\n",
      "Key 20151231 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.925385352090802\n",
      "Key 20151212 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9781912431731926\n",
      "Key 20151211 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9899781954842789\n",
      "Key 20151221 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867129288367005\n",
      "Key 20151201 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9695523681747558\n",
      "Key 20151220 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.938288391111464\n",
      "Key 20151230 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9489095185259758\n",
      "Current Layer Attributes - hidden layers:1 features count:42\n",
      "Key 20140115 | Training Loss: 0.616990 | Validation Loss: 0.588330\n",
      "Key 20140107 | Training Loss: 0.550074 | Validation Loss: 0.541367\n",
      "Key 20140109 | Training Loss: 0.515122 | Validation Loss: 0.482737\n",
      "Key 20140104 | Training Loss: 0.460425 | Validation Loss: 0.450408\n",
      "Key 20140119 | Training Loss: 0.426161 | Validation Loss: 0.412925\n",
      "Key 20140120 | Training Loss: 0.406649 | Validation Loss: 0.383472\n",
      "Key 20140123 | Training Loss: 0.374540 | Validation Loss: 0.367622\n",
      "Key 20140118 | Training Loss: 0.370033 | Validation Loss: 0.348939\n",
      "Key 20140108 | Training Loss: 0.357925 | Validation Loss: 0.344655\n",
      "Key 20140112 | Training Loss: 0.343647 | Validation Loss: 0.332625\n",
      "Key 20140126 | Training Loss: 0.343345 | Validation Loss: 0.332591\n",
      "Key 20140129 | Training Loss: 0.329661 | Validation Loss: 0.324410\n",
      "Key 20140130 | Training Loss: 0.333090 | Validation Loss: 0.327157\n",
      "Key 20140128 | Training Loss: 0.330284 | Validation Loss: 0.324690\n",
      "Key 20151224 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9828865688487585\n",
      "Key 20151204 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867503661554218\n",
      "Key 20151216 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9788375571943093\n",
      "Key 20151222 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867417614464858\n",
      "Key 20151214 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9849720497672902\n",
      "Key 20151202 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9535200295215056\n",
      "Key 20151227 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9881973019016337\n",
      "Key 20151203 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9062532636026961\n",
      "Key 20151223 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9652120631124332\n",
      "Key 20151205 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.986512289703779\n",
      "Key 20151229 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9727608594282435\n",
      "Key 20151208 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9833508070338446\n",
      "Key 20151219 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.6888589703530795\n",
      "Key 20151206 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9852853269571529\n",
      "Key 20151225 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9858456179148585\n",
      "Key 20151210 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9866921178466759\n",
      "Key 20151217 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.966389328069453\n",
      "Key 20151207 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9642177070037038\n",
      "Key 20151215 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9808085269898045\n",
      "Key 20151213 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9905969009923565\n",
      "Key 20151209 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9822953999241051\n",
      "Key 20151228 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.988685517789228\n",
      "Key 20151226 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9024700130748678\n",
      "Key 20151218 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9788305128278303\n",
      "Key 20151231 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.925385352090802\n",
      "Key 20151212 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9781912431731926\n",
      "Key 20151211 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9899781954842789\n",
      "Key 20151221 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867129288367005\n",
      "Key 20151201 Test Accuracy: 0.499978601284874 Quality score: -0.0011415491544824053, recall 0.999957202569748, precision 0.9695511047176423\n",
      "Key 20151220 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.938288391111464\n",
      "Key 20151230 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9489095185259758\n",
      "Current Layer Attributes - hidden layers:3 features count:1\n",
      "Key 20140115 | Training Loss: 0.821966 | Validation Loss: 1.088327\n",
      "Key 20140107 | Training Loss: 0.782770 | Validation Loss: 0.915970\n",
      "Key 20140109 | Training Loss: 0.744435 | Validation Loss: 0.880158\n",
      "Key 20140104 | Training Loss: 0.704344 | Validation Loss: 0.786574\n",
      "Key 20140119 | Training Loss: 0.701450 | Validation Loss: 0.742486\n",
      "Key 20140120 | Training Loss: 0.701879 | Validation Loss: 0.746969\n",
      "Key 20140123 | Training Loss: 0.685859 | Validation Loss: 0.733936\n",
      "Key 20140118 | Training Loss: 0.674485 | Validation Loss: 0.759629\n",
      "Key 20140108 | Training Loss: 0.684737 | Validation Loss: 0.728656\n",
      "Key 20140112 | Training Loss: 0.690186 | Validation Loss: 0.794316\n",
      "Key 20140126 | Training Loss: 0.668988 | Validation Loss: 0.757239\n",
      "Key 20140129 | Training Loss: 0.669730 | Validation Loss: 0.843470\n",
      "Key 20140130 | Training Loss: 0.672238 | Validation Loss: 0.757810\n",
      "Key 20140128 | Training Loss: 0.658027 | Validation Loss: 0.710989\n",
      "Key 20151224 Test Accuracy: 0.8636949527567079 Quality score: 0.26450402278898916, recall 0.8628666848483979, precision 0.9972737276935073\n",
      "Key 20151204 Test Accuracy: 0.7944532498334308 Quality score: 0.1483183060738831, recall 0.7172848780452399, precision 0.997602523659306\n",
      "Key 20151216 Test Accuracy: 0.8560131863416607 Quality score: 0.2545235262352264, recall 0.8115288602455104, precision 0.9973561584982981\n",
      "Key 20151222 Test Accuracy: 0.8282735193540712 Quality score: 0.18180051322346502, recall 0.7904486059338783, precision 0.9977290531970454\n",
      "Key 20151214 Test Accuracy: 0.7537748786205203 Quality score: 0.13307128972872934, recall 0.6938160902963645, precision 0.9959206536695852\n",
      "Key 20151202 Test Accuracy: 0.8846287222116506 Quality score: 0.38677737925373723, recall 0.809042200951177, precision 0.9976086576138539\n",
      "Key 20151227 Test Accuracy: 0.7743764413712291 Quality score: 0.13480695765261824, recall 0.744667149319819, precision 0.9968675872925781\n",
      "Key 20151203 Test Accuracy: 0.903171885850474 Quality score: 0.544840693747366, recall 0.8284098413697428, precision 0.9972521609219934\n",
      "Key 20151223 Test Accuracy: 0.8670101175307255 Quality score: 0.3242100294145291, recall 0.804616825970542, precision 0.9968476868237115\n",
      "Key 20151205 Test Accuracy: 0.7763578974967702 Quality score: 0.14288949095588876, recall 0.7330929472209248, precision 0.9966472614221707\n",
      "Key 20151229 Test Accuracy: 0.795949581743862 Quality score: 0.20458566011087978, recall 0.6840771987787431, precision 0.9962409850767108\n",
      "Key 20151208 Test Accuracy: 0.8473742454248839 Quality score: 0.24204775804329637, recall 0.8508570881348357, precision 0.9969032373885465\n",
      "Key 20151219 Test Accuracy: 0.5158970923243756 Quality score: 0.03590497698398688, recall 0.796137811426475, precision 0.6975261383094986\n",
      "Key 20151206 Test Accuracy: 0.8400312034211452 Quality score: 0.21969086900808144, recall 0.8432832991165016, precision 0.9971177126167327\n",
      "Key 20151225 Test Accuracy: 0.8446071672860387 Quality score: 0.22160474965733573, recall 0.8489101520625719, precision 0.9973063423394009\n",
      "Key 20151210 Test Accuracy: 0.7616071735690677 Quality score: 0.13179500701985072, recall 0.7144632604965823, precision 0.9964026620300978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151217 Test Accuracy: 0.7494804689437342 Quality score: 0.19446621504937442, recall 0.7069474711335176, precision 0.9898713423659342\n",
      "Key 20151207 Test Accuracy: 0.8587720027986515 Quality score: 0.3109147094708776, recall 0.7830410816206949, precision 0.996905543084096\n",
      "Key 20151215 Test Accuracy: 0.7542257173205992 Quality score: 0.15001539245402998, recall 0.6935187999106597, precision 0.9948056177060683\n",
      "Key 20151213 Test Accuracy: 0.5593673400786505 Quality score: 0.024621770495448173, recall 0.6838374740423828, precision 0.9922168535614067\n",
      "Key 20151209 Test Accuracy: 0.7780794078373164 Quality score: 0.16242153518903818, recall 0.7245322379180241, precision 0.9958289577704098\n",
      "Key 20151228 Test Accuracy: 0.6962500810887696 Quality score: 0.08795666836923577, recall 0.6694898231537355, precision 0.9952875726090158\n",
      "Key 20151226 Test Accuracy: 0.5713795361210716 Quality score: 0.08841791332050997, recall 0.6572579502184204, precision 0.9220016082142636\n",
      "Key 20151218 Test Accuracy: 0.8383673351760151 Quality score: 0.24168574686200364, recall 0.8101938237626534, precision 0.9964500860585198\n",
      "Key 20151231 Test Accuracy: 0.8133693882445195 Quality score: 0.35580838478564497, recall 0.7358840867531214, precision 0.9881822663567857\n",
      "Key 20151212 Test Accuracy: 0.7487086096513694 Quality score: 0.1576118148155407, recall 0.7045479493536726, precision 0.9934881585900263\n",
      "Key 20151211 Test Accuracy: 0.758625659957846 Quality score: 0.11749549317084822, recall 0.745435282179843, precision 0.9969107592587418\n",
      "Key 20151221 Test Accuracy: 0.8175684893266997 Quality score: 0.17712673874405482, recall 0.7937942224696539, precision 0.9973157491989095\n",
      "Key 20151201 Test Accuracy: 0.8902540378586258 Quality score: 0.37542951579624895, recall 0.873633454275701, precision 0.9966636648692048\n",
      "Key 20151220 Test Accuracy: 0.7656408622515266 Quality score: 0.27684744454489263, recall 0.7245110586941116, precision 0.9827612181211665\n",
      "Key 20151230 Test Accuracy: 0.907073845982482 Quality score: 0.45453833826134193, recall 0.8489456124747746, precision 0.9977979295231324\n",
      "Current Layer Attributes - hidden layers:3 features count:4\n",
      "Key 20140115 | Training Loss: 0.649585 | Validation Loss: 0.616536\n",
      "Key 20140107 | Training Loss: 0.639465 | Validation Loss: 0.580289\n",
      "Key 20140109 | Training Loss: 0.621764 | Validation Loss: 0.528968\n",
      "Key 20140104 | Training Loss: 0.578131 | Validation Loss: 0.504832\n",
      "Key 20140119 | Training Loss: 0.610194 | Validation Loss: 0.478435\n",
      "Key 20140120 | Training Loss: 0.564932 | Validation Loss: 0.441144\n",
      "Key 20140123 | Training Loss: 0.560081 | Validation Loss: 0.415543\n",
      "Key 20140118 | Training Loss: 0.538438 | Validation Loss: 0.389071\n",
      "Key 20140108 | Training Loss: 0.540132 | Validation Loss: 0.380750\n",
      "Key 20140112 | Training Loss: 0.531588 | Validation Loss: 0.357250\n",
      "Key 20140126 | Training Loss: 0.511316 | Validation Loss: 0.354882\n",
      "Key 20140129 | Training Loss: 0.508865 | Validation Loss: 0.338674\n",
      "Key 20140130 | Training Loss: 0.511138 | Validation Loss: 0.340405\n",
      "Key 20140128 | Training Loss: 0.504063 | Validation Loss: 0.335972\n",
      "Key 20151224 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9828865688487585\n",
      "Key 20151204 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867503661554218\n",
      "Key 20151216 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9788375571943093\n",
      "Key 20151222 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867417614464858\n",
      "Key 20151214 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9849720497672902\n",
      "Key 20151202 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9535200295215056\n",
      "Key 20151227 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9881973019016337\n",
      "Key 20151203 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9062532636026961\n",
      "Key 20151223 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9652120631124332\n",
      "Key 20151205 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.986512289703779\n",
      "Key 20151229 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9727608594282435\n",
      "Key 20151208 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9833508070338446\n",
      "Key 20151219 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.6888589703530795\n",
      "Key 20151206 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9852853269571529\n",
      "Key 20151225 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9858456179148585\n",
      "Key 20151210 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9866921178466759\n",
      "Key 20151217 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.966389328069453\n",
      "Key 20151207 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9642177070037038\n",
      "Key 20151215 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9808085269898045\n",
      "Key 20151213 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9905969009923565\n",
      "Key 20151209 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9822953999241051\n",
      "Key 20151228 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.988685517789228\n",
      "Key 20151226 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9024700130748678\n",
      "Key 20151218 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9788305128278303\n",
      "Key 20151231 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.925385352090802\n",
      "Key 20151212 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9781912431731926\n",
      "Key 20151211 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9899781954842789\n",
      "Key 20151221 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867129288367005\n",
      "Key 20151201 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9695523681747558\n",
      "Key 20151220 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.938288391111464\n",
      "Key 20151230 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9489095185259758\n",
      "Current Layer Attributes - hidden layers:3 features count:8\n",
      "Key 20140115 | Training Loss: 0.480156 | Validation Loss: 0.485446\n",
      "Key 20140107 | Training Loss: 0.467844 | Validation Loss: 0.443483\n",
      "Key 20140109 | Training Loss: 0.436166 | Validation Loss: 0.396179\n",
      "Key 20140104 | Training Loss: 0.419228 | Validation Loss: 0.364101\n",
      "Key 20140119 | Training Loss: 0.402241 | Validation Loss: 0.341009\n",
      "Key 20140120 | Training Loss: 0.375279 | Validation Loss: 0.331392\n",
      "Key 20140123 | Training Loss: 0.377360 | Validation Loss: 0.321705\n",
      "Key 20140118 | Training Loss: 0.359123 | Validation Loss: 0.321853\n",
      "Key 20140108 | Training Loss: 0.363142 | Validation Loss: 0.319187\n",
      "Key 20140112 | Training Loss: 0.365963 | Validation Loss: 0.320410\n",
      "Key 20140126 | Training Loss: 0.345201 | Validation Loss: 0.322016\n",
      "Key 20140129 | Training Loss: 0.339826 | Validation Loss: 0.318388\n",
      "Key 20140130 | Training Loss: 0.341174 | Validation Loss: 0.320927\n",
      "Key 20140128 | Training Loss: 0.336147 | Validation Loss: 0.320167\n",
      "Key 20151224 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9828865688487585\n",
      "Key 20151204 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867503661554218\n",
      "Key 20151216 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9788375571943093\n",
      "Key 20151222 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867417614464858\n",
      "Key 20151214 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9849720497672902\n",
      "Key 20151202 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9535200295215056\n",
      "Key 20151227 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9881973019016337\n",
      "Key 20151203 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9062532636026961\n",
      "Key 20151223 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9652120631124332\n",
      "Key 20151205 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.986512289703779\n",
      "Key 20151229 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9727608594282435\n",
      "Key 20151208 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9833508070338446\n",
      "Key 20151219 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.6888589703530795\n",
      "Key 20151206 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9852853269571529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151225 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9858456179148585\n",
      "Key 20151210 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9866921178466759\n",
      "Key 20151217 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.966389328069453\n",
      "Key 20151207 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9642177070037038\n",
      "Key 20151215 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9808085269898045\n",
      "Key 20151213 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9905969009923565\n",
      "Key 20151209 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9822953999241051\n",
      "Key 20151228 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.988685517789228\n",
      "Key 20151226 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9024700130748678\n",
      "Key 20151218 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9788305128278303\n",
      "Key 20151231 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.925385352090802\n",
      "Key 20151212 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9781912431731926\n",
      "Key 20151211 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9899781954842789\n",
      "Key 20151221 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867129288367005\n",
      "Key 20151201 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9695523681747558\n",
      "Key 20151220 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.938288391111464\n",
      "Key 20151230 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9489095185259758\n",
      "Current Layer Attributes - hidden layers:3 features count:16\n",
      "Key 20140115 | Training Loss: 0.918825 | Validation Loss: 0.786707\n",
      "Key 20140107 | Training Loss: 0.830777 | Validation Loss: 0.732849\n",
      "Key 20140109 | Training Loss: 0.777713 | Validation Loss: 0.706134\n",
      "Key 20140104 | Training Loss: 0.737168 | Validation Loss: 0.690598\n",
      "Key 20140119 | Training Loss: 0.699615 | Validation Loss: 0.674841\n",
      "Key 20140120 | Training Loss: 0.690034 | Validation Loss: 0.665428\n",
      "Key 20140123 | Training Loss: 0.669237 | Validation Loss: 0.651608\n",
      "Key 20140118 | Training Loss: 0.644890 | Validation Loss: 0.637694\n",
      "Key 20140108 | Training Loss: 0.628338 | Validation Loss: 0.620244\n",
      "Key 20140112 | Training Loss: 0.598363 | Validation Loss: 0.591194\n",
      "Key 20140126 | Training Loss: 0.572544 | Validation Loss: 0.561200\n",
      "Key 20140129 | Training Loss: 0.545022 | Validation Loss: 0.511039\n",
      "Key 20140130 | Training Loss: 0.506667 | Validation Loss: 0.472924\n",
      "Key 20140128 | Training Loss: 0.490048 | Validation Loss: 0.435736\n",
      "Key 20151224 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9828865688487585\n",
      "Key 20151204 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867503661554218\n",
      "Key 20151216 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9788375571943093\n",
      "Key 20151222 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867417614464858\n",
      "Key 20151214 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9849720497672902\n",
      "Key 20151202 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9535200295215056\n",
      "Key 20151227 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9881973019016337\n",
      "Key 20151203 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9062532636026961\n",
      "Key 20151223 Test Accuracy: 0.49988224994752445 Quality score: -0.0028625906872555514, recall 0.9997644998950489, precision 0.9652041537541456\n",
      "Key 20151205 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.986512289703779\n",
      "Key 20151229 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9727608594282435\n",
      "Key 20151208 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9833508070338446\n",
      "Key 20151219 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.6888589703530795\n",
      "Key 20151206 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9852853269571529\n",
      "Key 20151225 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9858456179148585\n",
      "Key 20151210 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9866921178466759\n",
      "Key 20151217 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.966389328069453\n",
      "Key 20151207 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9642177070037038\n",
      "Key 20151215 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9808085269898045\n",
      "Key 20151213 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9905969009923565\n",
      "Key 20151209 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9822953999241051\n",
      "Key 20151228 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.988685517789228\n",
      "Key 20151226 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9024700130748678\n",
      "Key 20151218 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9788305128278303\n",
      "Key 20151231 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.925385352090802\n",
      "Key 20151212 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9781912431731926\n",
      "Key 20151211 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9899781954842789\n",
      "Key 20151221 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867129288367005\n",
      "Key 20151201 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9695523681747558\n",
      "Key 20151220 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.938288391111464\n",
      "Key 20151230 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9489095185259758\n",
      "Current Layer Attributes - hidden layers:3 features count:42\n",
      "Key 20140115 | Training Loss: 0.604644 | Validation Loss: 0.576940\n",
      "Key 20140107 | Training Loss: 0.491124 | Validation Loss: 0.490644\n",
      "Key 20140109 | Training Loss: 0.427552 | Validation Loss: 0.408403\n",
      "Key 20140104 | Training Loss: 0.385031 | Validation Loss: 0.363055\n",
      "Key 20140119 | Training Loss: 0.341492 | Validation Loss: 0.334996\n",
      "Key 20140120 | Training Loss: 0.333017 | Validation Loss: 0.326812\n",
      "Key 20140123 | Training Loss: 0.331855 | Validation Loss: 0.318518\n",
      "Key 20140118 | Training Loss: 0.336972 | Validation Loss: 0.320050\n",
      "Key 20140108 | Training Loss: 0.333335 | Validation Loss: 0.319854\n",
      "Key 20140112 | Training Loss: 0.325427 | Validation Loss: 0.320723\n",
      "Key 20140126 | Training Loss: 0.322687 | Validation Loss: 0.322064\n",
      "Key 20140129 | Training Loss: 0.317287 | Validation Loss: 0.319262\n",
      "Key 20140130 | Training Loss: 0.321986 | Validation Loss: 0.320676\n",
      "Key 20140128 | Training Loss: 0.313456 | Validation Loss: 0.319974\n",
      "Key 20151224 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9828865688487585\n",
      "Key 20151204 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867503661554218\n",
      "Key 20151216 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9788375571943093\n",
      "Key 20151222 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867417614464858\n",
      "Key 20151214 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9849720497672902\n",
      "Key 20151202 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9535200295215056\n",
      "Key 20151227 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9881973019016337\n",
      "Key 20151203 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9062532636026961\n",
      "Key 20151223 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9652120631124332\n",
      "Key 20151205 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.986512289703779\n",
      "Key 20151229 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9727608594282435\n",
      "Key 20151208 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9833508070338446\n",
      "Key 20151219 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.6888589703530795\n",
      "Key 20151206 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9852853269571529\n",
      "Key 20151225 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9858456179148585\n",
      "Key 20151210 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9866921178466759\n",
      "Key 20151217 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.966389328069453\n",
      "Key 20151207 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9642177070037038\n",
      "Key 20151215 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9808085269898045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151213 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9905969009923565\n",
      "Key 20151209 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9822953999241051\n",
      "Key 20151228 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.988685517789228\n",
      "Key 20151226 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9024700130748678\n",
      "Key 20151218 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9788305128278303\n",
      "Key 20151231 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.925385352090802\n",
      "Key 20151212 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9781912431731926\n",
      "Key 20151211 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9899781954842789\n",
      "Key 20151221 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9867129288367005\n",
      "Key 20151201 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9695523681747558\n",
      "Key 20151220 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.938288391111464\n",
      "Key 20151230 Test Accuracy: 0.5 Quality score: 0.0, recall 1.0, precision 0.9489095185259758\n"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:32:17.574370Z",
     "start_time": "2017-07-21T21:31:24.687099Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_dense_only_nsl_kdd_scores.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:32:17.578518Z",
     "start_time": "2017-07-21T21:32:17.576195Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_results = pd.read_pickle(\"dataset/tf_dense_only_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:32:17.638679Z",
     "start_time": "2017-07-21T21:32:17.580033Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:32:17.643507Z",
     "start_time": "2017-07-21T21:32:17.640518Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#g = df_results.groupby(by=['no_of_features'])\n",
    "#idx = g['test_score'].transform(max) == df_results['test_score']\n",
    "#df_results[idx].sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:32:17.647754Z",
     "start_time": "2017-07-21T21:32:17.645332Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:32:17.723066Z",
     "start_time": "2017-07-21T21:32:17.649204Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "    \n",
    "    #print(cm)\n",
    "\n",
    "    label = [[\"\\n True Negative\", \"\\n False Positive \\n Type II Error\"],\n",
    "             [\"\\n False Negative \\n Type I Error\", \"\\n True Positive\"]\n",
    "            ]\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        \n",
    "        plt.text(j, i, \"{} {}\".format(cm[i, j].round(4), label[i][j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, ['Normal', 'Attack'], normalize = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:32:17.727928Z",
     "start_time": "2017-07-21T21:32:17.724612Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:32:17.788549Z",
     "start_time": "2017-07-21T21:32:17.729275Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "past_scores = pd.read_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:32:17.857429Z",
     "start_time": "2017-07-21T21:32:17.790160Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>20151203</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.903172</td>\n",
       "      <td>0.544841</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>20151203</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.903172</td>\n",
       "      <td>0.544841</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>20151230</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.907074</td>\n",
       "      <td>0.454538</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>20151230</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.907074</td>\n",
       "      <td>0.454538</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>20151202</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.884629</td>\n",
       "      <td>0.386777</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>20151202</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.884629</td>\n",
       "      <td>0.386777</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>20151201</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.890254</td>\n",
       "      <td>0.375430</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>20151201</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.890254</td>\n",
       "      <td>0.375430</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>20151231</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.813369</td>\n",
       "      <td>0.355808</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>20151231</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.813369</td>\n",
       "      <td>0.355808</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>20151223</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.867010</td>\n",
       "      <td>0.324210</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>20151223</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.867010</td>\n",
       "      <td>0.324210</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>20151207</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.858772</td>\n",
       "      <td>0.310915</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>20151207</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.858772</td>\n",
       "      <td>0.310915</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>20151220</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.765641</td>\n",
       "      <td>0.276847</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>20151220</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.765641</td>\n",
       "      <td>0.276847</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>20151224</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.863695</td>\n",
       "      <td>0.264504</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>20151224</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.863695</td>\n",
       "      <td>0.264504</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>20151216</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.856013</td>\n",
       "      <td>0.254524</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>20151216</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.856013</td>\n",
       "      <td>0.254524</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>20151208</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.847374</td>\n",
       "      <td>0.242048</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>20151208</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.847374</td>\n",
       "      <td>0.242048</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>20151218</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.838367</td>\n",
       "      <td>0.241686</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>20151218</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.838367</td>\n",
       "      <td>0.241686</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>20151225</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.844607</td>\n",
       "      <td>0.221605</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>20151225</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.844607</td>\n",
       "      <td>0.221605</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>20151206</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.840031</td>\n",
       "      <td>0.219691</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>20151206</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.840031</td>\n",
       "      <td>0.219691</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>20151229</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.795950</td>\n",
       "      <td>0.204586</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>20151229</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.795950</td>\n",
       "      <td>0.204586</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>20151217</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.749480</td>\n",
       "      <td>0.194466</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>20151217</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.749480</td>\n",
       "      <td>0.194466</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>20151222</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.828274</td>\n",
       "      <td>0.181801</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>20151222</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.828274</td>\n",
       "      <td>0.181801</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>20151221</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.817568</td>\n",
       "      <td>0.177127</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>20151221</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.817568</td>\n",
       "      <td>0.177127</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>20151209</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.778079</td>\n",
       "      <td>0.162422</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>20151209</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.778079</td>\n",
       "      <td>0.162422</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>20151212</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.748709</td>\n",
       "      <td>0.157612</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>20151212</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.748709</td>\n",
       "      <td>0.157612</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>20151215</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.754226</td>\n",
       "      <td>0.150015</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>20151215</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.754226</td>\n",
       "      <td>0.150015</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>20151204</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.794453</td>\n",
       "      <td>0.148318</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>20151204</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.794453</td>\n",
       "      <td>0.148318</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>20151205</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.776358</td>\n",
       "      <td>0.142889</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>20151205</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.776358</td>\n",
       "      <td>0.142889</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20151203</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.328882</td>\n",
       "      <td>0.513021</td>\n",
       "      <td>0.142743</td>\n",
       "      <td>50.331757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>20151203</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.328882</td>\n",
       "      <td>0.513021</td>\n",
       "      <td>0.142743</td>\n",
       "      <td>50.331757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.774376</td>\n",
       "      <td>0.134807</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.774376</td>\n",
       "      <td>0.134807</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>20151228</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497969</td>\n",
       "      <td>-0.006792</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>20151228</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497969</td>\n",
       "      <td>-0.006792</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>20151230</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.328882</td>\n",
       "      <td>0.499526</td>\n",
       "      <td>-0.006963</td>\n",
       "      <td>50.331757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>20151230</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.328882</td>\n",
       "      <td>0.499526</td>\n",
       "      <td>-0.006963</td>\n",
       "      <td>50.331757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>20151221</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498111</td>\n",
       "      <td>-0.007098</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>20151221</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498111</td>\n",
       "      <td>-0.007098</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>20151210</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497823</td>\n",
       "      <td>-0.007628</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>20151210</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497823</td>\n",
       "      <td>-0.007628</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>20151206</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498017</td>\n",
       "      <td>-0.007653</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>20151206</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498017</td>\n",
       "      <td>-0.007653</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>20151217</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.499088</td>\n",
       "      <td>-0.007838</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>20151217</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.499088</td>\n",
       "      <td>-0.007838</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>20151209</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498182</td>\n",
       "      <td>-0.008037</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>20151209</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498182</td>\n",
       "      <td>-0.008037</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>20151222</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497484</td>\n",
       "      <td>-0.008188</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>20151222</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497484</td>\n",
       "      <td>-0.008188</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>20151216</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498402</td>\n",
       "      <td>-0.008236</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>20151216</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498402</td>\n",
       "      <td>-0.008236</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>20151214</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497601</td>\n",
       "      <td>-0.008511</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>20151214</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497601</td>\n",
       "      <td>-0.008511</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>20151202</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.499013</td>\n",
       "      <td>-0.009587</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>20151202</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.499013</td>\n",
       "      <td>-0.009587</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>20151220</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.499225</td>\n",
       "      <td>-0.009786</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>20151220</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.499225</td>\n",
       "      <td>-0.009786</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>20151224</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497067</td>\n",
       "      <td>-0.010048</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>20151224</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497067</td>\n",
       "      <td>-0.010048</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>20151215</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497155</td>\n",
       "      <td>-0.010479</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>20151215</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497155</td>\n",
       "      <td>-0.010479</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>20151201</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497924</td>\n",
       "      <td>-0.011265</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>20151201</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497924</td>\n",
       "      <td>-0.011265</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>20151207</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498188</td>\n",
       "      <td>-0.011408</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>20151207</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498188</td>\n",
       "      <td>-0.011408</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>20151218</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.496883</td>\n",
       "      <td>-0.011522</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>20151218</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.496883</td>\n",
       "      <td>-0.011522</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>20151208</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.495873</td>\n",
       "      <td>-0.011771</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>20151208</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.495873</td>\n",
       "      <td>-0.011771</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>20151223</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497864</td>\n",
       "      <td>-0.012010</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>20151223</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497864</td>\n",
       "      <td>-0.012010</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>20151229</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497069</td>\n",
       "      <td>-0.012673</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>20151229</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497069</td>\n",
       "      <td>-0.012673</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>20151226</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498819</td>\n",
       "      <td>-0.015089</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>20151226</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498819</td>\n",
       "      <td>-0.015089</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>20151203</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498699</td>\n",
       "      <td>-0.015636</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>20151203</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498699</td>\n",
       "      <td>-0.015636</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>20151231</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498299</td>\n",
       "      <td>-0.015957</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>20151231</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498299</td>\n",
       "      <td>-0.015957</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>20151230</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497255</td>\n",
       "      <td>-0.016791</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>20151230</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.497255</td>\n",
       "      <td>-0.016791</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>20151219</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498092</td>\n",
       "      <td>-0.034501</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>20151219</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498092</td>\n",
       "      <td>-0.034501</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>620 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          key  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "162  20151203               1              3     0.710989    0.903172   \n",
       "472  20151203               1              3     0.710989    0.903172   \n",
       "495  20151230               1              3     0.710989    0.907074   \n",
       "185  20151230               1              3     0.710989    0.907074   \n",
       "160  20151202               1              3     0.710989    0.884629   \n",
       "470  20151202               1              3     0.710989    0.884629   \n",
       "493  20151201               1              3     0.710989    0.890254   \n",
       "183  20151201               1              3     0.710989    0.890254   \n",
       "179  20151231               1              3     0.710989    0.813369   \n",
       "489  20151231               1              3     0.710989    0.813369   \n",
       "163  20151223               1              3     0.710989    0.867010   \n",
       "473  20151223               1              3     0.710989    0.867010   \n",
       "172  20151207               1              3     0.710989    0.858772   \n",
       "482  20151207               1              3     0.710989    0.858772   \n",
       "184  20151220               1              3     0.710989    0.765641   \n",
       "494  20151220               1              3     0.710989    0.765641   \n",
       "465  20151224               1              3     0.710989    0.863695   \n",
       "155  20151224               1              3     0.710989    0.863695   \n",
       "467  20151216               1              3     0.710989    0.856013   \n",
       "157  20151216               1              3     0.710989    0.856013   \n",
       "476  20151208               1              3     0.710989    0.847374   \n",
       "166  20151208               1              3     0.710989    0.847374   \n",
       "178  20151218               1              3     0.710989    0.838367   \n",
       "488  20151218               1              3     0.710989    0.838367   \n",
       "479  20151225               1              3     0.710989    0.844607   \n",
       "169  20151225               1              3     0.710989    0.844607   \n",
       "478  20151206               1              3     0.710989    0.840031   \n",
       "168  20151206               1              3     0.710989    0.840031   \n",
       "475  20151229               1              3     0.710989    0.795950   \n",
       "165  20151229               1              3     0.710989    0.795950   \n",
       "171  20151217               1              3     0.710989    0.749480   \n",
       "481  20151217               1              3     0.710989    0.749480   \n",
       "468  20151222               1              3     0.710989    0.828274   \n",
       "158  20151222               1              3     0.710989    0.828274   \n",
       "182  20151221               1              3     0.710989    0.817568   \n",
       "492  20151221               1              3     0.710989    0.817568   \n",
       "485  20151209               1              3     0.710989    0.778079   \n",
       "175  20151209               1              3     0.710989    0.778079   \n",
       "490  20151212               1              3     0.710989    0.748709   \n",
       "180  20151212               1              3     0.710989    0.748709   \n",
       "173  20151215               1              3     0.710989    0.754226   \n",
       "483  20151215               1              3     0.710989    0.754226   \n",
       "156  20151204               1              3     0.710989    0.794453   \n",
       "466  20151204               1              3     0.710989    0.794453   \n",
       "474  20151205               1              3     0.710989    0.776358   \n",
       "164  20151205               1              3     0.710989    0.776358   \n",
       "38   20151203               4              1     0.328882    0.513021   \n",
       "348  20151203               4              1     0.328882    0.513021   \n",
       "161  20151227               1              3     0.710989    0.774376   \n",
       "471  20151227               1              3     0.710989    0.774376   \n",
       "..        ...             ...            ...          ...         ...   \n",
       "83   20151228               8              1     0.327700    0.497969   \n",
       "393  20151228               8              1     0.327700    0.497969   \n",
       "61   20151230               4              1     0.328882    0.499526   \n",
       "371  20151230               4              1     0.328882    0.499526   \n",
       "89   20151221               8              1     0.327700    0.498111   \n",
       "399  20151221               8              1     0.327700    0.498111   \n",
       "77   20151210               8              1     0.327700    0.497823   \n",
       "387  20151210               8              1     0.327700    0.497823   \n",
       "75   20151206               8              1     0.327700    0.498017   \n",
       "385  20151206               8              1     0.327700    0.498017   \n",
       "388  20151217               8              1     0.327700    0.499088   \n",
       "78   20151217               8              1     0.327700    0.499088   \n",
       "392  20151209               8              1     0.327700    0.498182   \n",
       "82   20151209               8              1     0.327700    0.498182   \n",
       "375  20151222               8              1     0.327700    0.497484   \n",
       "65   20151222               8              1     0.327700    0.497484   \n",
       "374  20151216               8              1     0.327700    0.498402   \n",
       "64   20151216               8              1     0.327700    0.498402   \n",
       "376  20151214               8              1     0.327700    0.497601   \n",
       "66   20151214               8              1     0.327700    0.497601   \n",
       "377  20151202               8              1     0.327700    0.499013   \n",
       "67   20151202               8              1     0.327700    0.499013   \n",
       "401  20151220               8              1     0.327700    0.499225   \n",
       "91   20151220               8              1     0.327700    0.499225   \n",
       "372  20151224               8              1     0.327700    0.497067   \n",
       "62   20151224               8              1     0.327700    0.497067   \n",
       "80   20151215               8              1     0.327700    0.497155   \n",
       "390  20151215               8              1     0.327700    0.497155   \n",
       "90   20151201               8              1     0.327700    0.497924   \n",
       "400  20151201               8              1     0.327700    0.497924   \n",
       "389  20151207               8              1     0.327700    0.498188   \n",
       "79   20151207               8              1     0.327700    0.498188   \n",
       "395  20151218               8              1     0.327700    0.496883   \n",
       "85   20151218               8              1     0.327700    0.496883   \n",
       "383  20151208               8              1     0.327700    0.495873   \n",
       "73   20151208               8              1     0.327700    0.495873   \n",
       "70   20151223               8              1     0.327700    0.497864   \n",
       "380  20151223               8              1     0.327700    0.497864   \n",
       "72   20151229               8              1     0.327700    0.497069   \n",
       "382  20151229               8              1     0.327700    0.497069   \n",
       "394  20151226               8              1     0.327700    0.498819   \n",
       "84   20151226               8              1     0.327700    0.498819   \n",
       "69   20151203               8              1     0.327700    0.498699   \n",
       "379  20151203               8              1     0.327700    0.498699   \n",
       "86   20151231               8              1     0.327700    0.498299   \n",
       "396  20151231               8              1     0.327700    0.498299   \n",
       "92   20151230               8              1     0.327700    0.497255   \n",
       "402  20151230               8              1     0.327700    0.497255   \n",
       "74   20151219               8              1     0.327700    0.498092   \n",
       "384  20151219               8              1     0.327700    0.498092   \n",
       "\n",
       "     quality_score  time_taken  \n",
       "162       0.544841   55.044972  \n",
       "472       0.544841   55.044972  \n",
       "495       0.454538   55.044972  \n",
       "185       0.454538   55.044972  \n",
       "160       0.386777   55.044972  \n",
       "470       0.386777   55.044972  \n",
       "493       0.375430   55.044972  \n",
       "183       0.375430   55.044972  \n",
       "179       0.355808   55.044972  \n",
       "489       0.355808   55.044972  \n",
       "163       0.324210   55.044972  \n",
       "473       0.324210   55.044972  \n",
       "172       0.310915   55.044972  \n",
       "482       0.310915   55.044972  \n",
       "184       0.276847   55.044972  \n",
       "494       0.276847   55.044972  \n",
       "465       0.264504   55.044972  \n",
       "155       0.264504   55.044972  \n",
       "467       0.254524   55.044972  \n",
       "157       0.254524   55.044972  \n",
       "476       0.242048   55.044972  \n",
       "166       0.242048   55.044972  \n",
       "178       0.241686   55.044972  \n",
       "488       0.241686   55.044972  \n",
       "479       0.221605   55.044972  \n",
       "169       0.221605   55.044972  \n",
       "478       0.219691   55.044972  \n",
       "168       0.219691   55.044972  \n",
       "475       0.204586   55.044972  \n",
       "165       0.204586   55.044972  \n",
       "171       0.194466   55.044972  \n",
       "481       0.194466   55.044972  \n",
       "468       0.181801   55.044972  \n",
       "158       0.181801   55.044972  \n",
       "182       0.177127   55.044972  \n",
       "492       0.177127   55.044972  \n",
       "485       0.162422   55.044972  \n",
       "175       0.162422   55.044972  \n",
       "490       0.157612   55.044972  \n",
       "180       0.157612   55.044972  \n",
       "173       0.150015   55.044972  \n",
       "483       0.150015   55.044972  \n",
       "156       0.148318   55.044972  \n",
       "466       0.148318   55.044972  \n",
       "474       0.142889   55.044972  \n",
       "164       0.142889   55.044972  \n",
       "38        0.142743   50.331757  \n",
       "348       0.142743   50.331757  \n",
       "161       0.134807   55.044972  \n",
       "471       0.134807   55.044972  \n",
       "..             ...         ...  \n",
       "83       -0.006792   50.228796  \n",
       "393      -0.006792   50.228796  \n",
       "61       -0.006963   50.331757  \n",
       "371      -0.006963   50.331757  \n",
       "89       -0.007098   50.228796  \n",
       "399      -0.007098   50.228796  \n",
       "77       -0.007628   50.228796  \n",
       "387      -0.007628   50.228796  \n",
       "75       -0.007653   50.228796  \n",
       "385      -0.007653   50.228796  \n",
       "388      -0.007838   50.228796  \n",
       "78       -0.007838   50.228796  \n",
       "392      -0.008037   50.228796  \n",
       "82       -0.008037   50.228796  \n",
       "375      -0.008188   50.228796  \n",
       "65       -0.008188   50.228796  \n",
       "374      -0.008236   50.228796  \n",
       "64       -0.008236   50.228796  \n",
       "376      -0.008511   50.228796  \n",
       "66       -0.008511   50.228796  \n",
       "377      -0.009587   50.228796  \n",
       "67       -0.009587   50.228796  \n",
       "401      -0.009786   50.228796  \n",
       "91       -0.009786   50.228796  \n",
       "372      -0.010048   50.228796  \n",
       "62       -0.010048   50.228796  \n",
       "80       -0.010479   50.228796  \n",
       "390      -0.010479   50.228796  \n",
       "90       -0.011265   50.228796  \n",
       "400      -0.011265   50.228796  \n",
       "389      -0.011408   50.228796  \n",
       "79       -0.011408   50.228796  \n",
       "395      -0.011522   50.228796  \n",
       "85       -0.011522   50.228796  \n",
       "383      -0.011771   50.228796  \n",
       "73       -0.011771   50.228796  \n",
       "70       -0.012010   50.228796  \n",
       "380      -0.012010   50.228796  \n",
       "72       -0.012673   50.228796  \n",
       "382      -0.012673   50.228796  \n",
       "394      -0.015089   50.228796  \n",
       "84       -0.015089   50.228796  \n",
       "69       -0.015636   50.228796  \n",
       "379      -0.015636   50.228796  \n",
       "86       -0.015957   50.228796  \n",
       "396      -0.015957   50.228796  \n",
       "92       -0.016791   50.228796  \n",
       "402      -0.016791   50.228796  \n",
       "74       -0.034501   50.228796  \n",
       "384      -0.034501   50.228796  \n",
       "\n",
       "[620 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores.sort_values(by='quality_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:32:17.879416Z",
     "start_time": "2017-07-21T21:32:17.859097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>20151203</td>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.903172</td>\n",
       "      <td>0.544841</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>1</th>\n",
       "      <td>20151203</td>\n",
       "      <td>0.328882</td>\n",
       "      <td>0.513021</td>\n",
       "      <td>0.142743</td>\n",
       "      <td>50.331757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>20151226</td>\n",
       "      <td>0.320999</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.379667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <td>20151224</td>\n",
       "      <td>0.335972</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.875265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>20151230</td>\n",
       "      <td>0.320167</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.000915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16</th>\n",
       "      <th>1</th>\n",
       "      <td>20151221</td>\n",
       "      <td>0.321321</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.474659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20151217</td>\n",
       "      <td>0.435736</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.303406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">42</th>\n",
       "      <th>1</th>\n",
       "      <td>20151230</td>\n",
       "      <td>0.324690</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52.110761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20151212</td>\n",
       "      <td>0.319974</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.970073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>20151204</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.499597</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   key  train_score  test_score  \\\n",
       "no_of_features hidden_layers                                      \n",
       "1              3              20151203     0.710989    0.903172   \n",
       "4              1              20151203     0.328882    0.513021   \n",
       "1              1              20151226     0.320999    0.500000   \n",
       "4              3              20151224     0.335972    0.500000   \n",
       "8              3              20151230     0.320167    0.500000   \n",
       "16             1              20151221     0.321321    0.500000   \n",
       "               3              20151217     0.435736    0.500000   \n",
       "42             1              20151230     0.324690    0.500000   \n",
       "               3              20151212     0.319974    0.500000   \n",
       "8              1              20151204     0.327700    0.499597   \n",
       "\n",
       "                              quality_score  time_taken  \n",
       "no_of_features hidden_layers                             \n",
       "1              3                   0.544841   55.044972  \n",
       "4              1                   0.142743   50.331757  \n",
       "1              1                   0.000000   55.379667  \n",
       "4              3                   0.000000   57.875265  \n",
       "8              3                   0.000000   58.000915  \n",
       "16             1                   0.000000   51.474659  \n",
       "               3                   0.000000   60.303406  \n",
       "42             1                   0.000000   52.110761  \n",
       "               3                   0.000000   67.970073  \n",
       "8              1                  -0.001877   50.228796  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.sort_values(by='quality_score', ascending=False).groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.first().sort_values(by='quality_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:32:54.343329Z",
     "start_time": "2017-07-21T21:32:17.881121Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#key_nof_hidden '20151201_16_1'\n",
    "Train.predictions = pd.read_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:33:34.233246Z",
     "start_time": "2017-07-21T21:33:34.208117Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = Train.predictions['20151203_1_3'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:33:34.782561Z",
     "start_time": "2017-07-21T21:33:34.776492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train.predictions['20151219_42_1'].loc[:,'Prediction']\n",
    "df.loc[:,'Prediction'].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:33:35.189952Z",
     "start_time": "2017-07-21T21:33:35.149753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90502349657054182"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics as me\n",
    "me.f1_score(df.loc[:,'Actual'].values.astype(int),\n",
    "            df.loc[:,'Prediction'].values.astype(int) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:33:35.478148Z",
     "start_time": "2017-07-21T21:33:35.465010Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0     23339\n",
       "1.0    225619\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:33:36.218030Z",
     "start_time": "2017-07-21T21:33:35.731333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGhCAYAAADiGPptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb0a6853a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:33:36.242343Z",
     "start_time": "2017-07-21T21:33:36.223430Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>0.710989</td>\n",
       "      <td>0.787561</td>\n",
       "      <td>0.217636</td>\n",
       "      <td>55.044972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>1</th>\n",
       "      <td>0.328882</td>\n",
       "      <td>0.500382</td>\n",
       "      <td>0.004359</td>\n",
       "      <td>50.331757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.335972</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.875265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>0.320167</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.000915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>1</th>\n",
       "      <td>0.321321</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.474659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">42</th>\n",
       "      <th>3</th>\n",
       "      <td>0.319974</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.970073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.324690</td>\n",
       "      <td>0.499999</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>52.110761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.320999</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>55.379667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>3</th>\n",
       "      <td>0.435736</td>\n",
       "      <td>0.499996</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>60.303406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.498130</td>\n",
       "      <td>-0.010115</td>\n",
       "      <td>50.228796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              train_score  test_score  quality_score  \\\n",
       "no_of_features hidden_layers                                           \n",
       "1              3                 0.710989    0.787561       0.217636   \n",
       "4              1                 0.328882    0.500382       0.004359   \n",
       "               3                 0.335972    0.500000       0.000000   \n",
       "8              3                 0.320167    0.500000       0.000000   \n",
       "16             1                 0.321321    0.500000       0.000000   \n",
       "42             3                 0.319974    0.500000       0.000000   \n",
       "               1                 0.324690    0.499999      -0.000037   \n",
       "1              1                 0.320999    0.500000      -0.000061   \n",
       "16             3                 0.435736    0.499996      -0.000092   \n",
       "8              1                 0.327700    0.498130      -0.010115   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "1              3               55.044972  \n",
       "4              1               50.331757  \n",
       "               3               57.875265  \n",
       "8              3               58.000915  \n",
       "16             1               51.474659  \n",
       "42             3               67.970073  \n",
       "               1               52.110761  \n",
       "1              1               55.379667  \n",
       "16             3               60.303406  \n",
       "8              1               50.228796  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.mean().sort_values(by='quality_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:33:36.265651Z",
     "start_time": "2017-07-21T21:33:36.247429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.960261e-07</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.470045e-02</td>\n",
       "      <td>0.117982</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.345350e-03</td>\n",
       "      <td>0.025767</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.085994e-04</td>\n",
       "      <td>0.005713</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.097447e-05</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">42</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.811690e-06</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              train_score    test_score  quality_score  \\\n",
       "no_of_features hidden_layers                                             \n",
       "1              1                      0.0  6.960261e-07       0.000266   \n",
       "               3                      0.0  9.470045e-02       0.117982   \n",
       "4              1                      0.0  2.345350e-03       0.025767   \n",
       "               3                      0.0  0.000000e+00       0.000000   \n",
       "8              1                      0.0  8.085994e-04       0.005713   \n",
       "               3                      0.0  0.000000e+00       0.000000   \n",
       "16             1                      0.0  0.000000e+00       0.000000   \n",
       "               3                      0.0  2.097447e-05       0.000510   \n",
       "42             1                      0.0  3.811690e-06       0.000203   \n",
       "               3                      0.0  0.000000e+00       0.000000   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "1              1                     0.0  \n",
       "               3                     0.0  \n",
       "4              1                     0.0  \n",
       "               3                     0.0  \n",
       "8              1                     0.0  \n",
       "               3                     0.0  \n",
       "16             1                     0.0  \n",
       "               3                     0.0  \n",
       "42             1                     0.0  \n",
       "               3                     0.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:34:03.153268Z",
     "start_time": "2017-07-21T21:34:03.061646Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1907: RuntimeWarning: invalid value encountered in multiply\n",
      "  lower_bound = self.a * scale + loc\n",
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1908: RuntimeWarning: invalid value encountered in multiply\n",
      "  upper_bound = self.b * scale + loc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "no_of_features  hidden_layers\n",
       "1               1                (-0.000582524413279, 0.000460736563856)\n",
       "                3                     (-0.0136042360783, 0.448877069061)\n",
       "4               1                    (-0.0461442902724, 0.0548624077983)\n",
       "                3                                             (nan, nan)\n",
       "8               1                   (-0.0213126393688, 0.00108259546053)\n",
       "                3                                             (nan, nan)\n",
       "16              1                                             (nan, nan)\n",
       "                3                 (-0.00109173671225, 0.000907053442103)\n",
       "42              1                (-0.000435364764629, 0.000361716432081)\n",
       "                3                                             (nan, nan)\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fn(x):\n",
    "    #print(x)\n",
    "    return stats.norm.interval(0.95, loc=x.quality_score.mean(), scale=x.quality_score.std())\n",
    "psg.apply(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

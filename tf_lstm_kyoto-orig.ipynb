{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:37:54.874995Z",
     "start_time": "2017-07-21T21:37:50.817571Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:37:55.191501Z",
     "start_time": "2017-07-21T21:37:54.876678Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'dataset/scores/tf_lstm_nsl_kdd-orig_all.pkl': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm dataset/scores/tf_lstm_nsl_kdd-orig_all.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:37:55.348191Z",
     "start_time": "2017-07-21T21:37:55.193383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20140115': {'x': 'dataset/Kyoto2016/2014/01/20140115_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140115_y.csv'}}\n",
      "----------------------------------------------------------------------------------------\n",
      "{'20151224': {'y': 'dataset/Kyoto2016/2015/12/20151224_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151224_x.csv'}, '20151204': {'y': 'dataset/Kyoto2016/2015/12/20151204_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151204_x.csv'}, '20151216': {'x': 'dataset/Kyoto2016/2015/12/20151216_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151216_y.csv'}, '20151222': {'x': 'dataset/Kyoto2016/2015/12/20151222_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151222_y.csv'}, '20151214': {'x': 'dataset/Kyoto2016/2015/12/20151214_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151214_y.csv'}, '20151202': {'x': 'dataset/Kyoto2016/2015/12/20151202_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151202_y.csv'}, '20151227': {'y': 'dataset/Kyoto2016/2015/12/20151227_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151227_x.csv'}, '20151203': {'y': 'dataset/Kyoto2016/2015/12/20151203_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151203_x.csv'}, '20151223': {'y': 'dataset/Kyoto2016/2015/12/20151223_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151223_x.csv'}, '20151205': {'x': 'dataset/Kyoto2016/2015/12/20151205_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151205_y.csv'}, '20151229': {'x': 'dataset/Kyoto2016/2015/12/20151229_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151229_y.csv'}, '20151208': {'x': 'dataset/Kyoto2016/2015/12/20151208_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151208_y.csv'}, '20151219': {'y': 'dataset/Kyoto2016/2015/12/20151219_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151219_x.csv'}, '20151206': {'y': 'dataset/Kyoto2016/2015/12/20151206_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151206_x.csv'}, '20151225': {'x': 'dataset/Kyoto2016/2015/12/20151225_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151225_y.csv'}, '20151210': {'x': 'dataset/Kyoto2016/2015/12/20151210_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151210_y.csv'}, '20151217': {'x': 'dataset/Kyoto2016/2015/12/20151217_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151217_y.csv'}, '20151207': {'x': 'dataset/Kyoto2016/2015/12/20151207_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151207_y.csv'}, '20151215': {'x': 'dataset/Kyoto2016/2015/12/20151215_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151215_y.csv'}, '20151213': {'y': 'dataset/Kyoto2016/2015/12/20151213_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151213_x.csv'}, '20151209': {'x': 'dataset/Kyoto2016/2015/12/20151209_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151209_y.csv'}, '20151228': {'y': 'dataset/Kyoto2016/2015/12/20151228_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151228_x.csv'}, '20151226': {'y': 'dataset/Kyoto2016/2015/12/20151226_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151226_x.csv'}, '20151218': {'x': 'dataset/Kyoto2016/2015/12/20151218_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151218_y.csv'}, '20151231': {'x': 'dataset/Kyoto2016/2015/12/20151231_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151231_y.csv'}, '20151212': {'x': 'dataset/Kyoto2016/2015/12/20151212_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151212_y.csv'}, '20151211': {'x': 'dataset/Kyoto2016/2015/12/20151211_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151211_y.csv'}, '20151221': {'y': 'dataset/Kyoto2016/2015/12/20151221_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151221_x.csv'}, '20151201': {'x': 'dataset/Kyoto2016/2015/12/20151201_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151201_y.csv'}, '20151220': {'x': 'dataset/Kyoto2016/2015/12/20151220_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151220_y.csv'}, '20151230': {'y': 'dataset/Kyoto2016/2015/12/20151230_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151230_x.csv'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class preprocess:\n",
    "    \n",
    "    paths = {}\n",
    "\n",
    "    def get_files(folder_path):\n",
    "        paths = {}\n",
    "        for path, subdirs, files in os.walk(folder_path):\n",
    "            for name in files:\n",
    "                if name.endswith(\"csv\"):\n",
    "                    key = name.split(\"_\")[0]\n",
    "\n",
    "                    if paths.get(key) is None:\n",
    "                        paths[key] = {}\n",
    "\n",
    "                    if name.endswith(\"_x.csv\"):\n",
    "                        x = os.path.join(path, name)\n",
    "                        paths[key]['x'] = x\n",
    "                    elif name.endswith(\"_y.csv\"):\n",
    "                        y = os.path.join(path, name)\n",
    "                        paths[key]['y'] = y\n",
    "        preprocess.paths = paths\n",
    "        return paths\n",
    "\n",
    "    def get_data(paths):\n",
    "        for key, value in paths.items():\n",
    "            x = pd.read_csv(value['x'])\n",
    "            y = pd.read_csv(value['y'])\n",
    "            #print(x.shape)\n",
    "            #print(x.values.shape)\n",
    "            #print(y.sum())\n",
    "            yield key, x.values, y.values\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "train_paths = preprocess.get_files(\"dataset/Kyoto2016/2014/01\")\n",
    "test_paths = preprocess.get_files(\"dataset/Kyoto2016/2015/12\")\n",
    "#test_paths.update(preprocess.get_files(\"dataset/Kyoto2016/2015/11\"))\n",
    "#test_paths.update(preprocess.get_files(\"dataset/Kyoto2016/2015/10\"))\n",
    "\n",
    "\n",
    "\n",
    "paths = {}\n",
    "keys = train_paths.keys()\n",
    "for key in list(keys)[0:1]:\n",
    "    paths.update({key: train_paths[key]})\n",
    "train_paths = paths\n",
    "\n",
    "print(train_paths)\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "#test_paths = test_paths.popitem()\n",
    "#test_paths = {test_paths[0]: test_paths[1]}\n",
    "print(test_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:38:10.490151Z",
     "start_time": "2017-07-21T21:37:55.349779Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.legacy_seq2seq.python.ops.seq2seq import basic_rnn_seq2seq\n",
    "from tensorflow.contrib.rnn import RNNCell, LSTMCell, MultiRNNCell\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import metrics as me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:38:10.778753Z",
     "start_time": "2017-07-21T21:38:10.491774Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 42\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 42\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 40\n",
    "\n",
    "    hidden_decoder_dim = 42\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x_input = tf.placeholder(\"float\", shape=[None, 1, input_dim])\n",
    "            self.y_input_ = tf.placeholder(\"float\", shape=[None, 1, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "            self.x_list = tf.unstack(self.x_input, axis= 1)\n",
    "            self.y_list_ = tf.unstack(self.y_input_, axis = 1)\n",
    "            self.y_ = self.y_list_[0]\n",
    "            \n",
    "            #GO = tf.fill((tf.shape(self.x)[0], 1), 0.5)\n",
    "            \n",
    "            #y_with_GO = tf.stack([self.y_, GO])\n",
    "            \n",
    "        with tf.variable_scope(\"lstm\"):\n",
    "            multi_cell = MultiRNNCell([LSTMCell(input_dim) for i in range(hidden_layers)] )\n",
    "            \n",
    "            self.y, states = basic_rnn_seq2seq(self.x_list, self.y_list_, multi_cell)\n",
    "            #self.y = tf.slice(self.y, [0, 0], [-1,2])\n",
    "            \n",
    "            #self.out = tf.squeeze(self.y)\n",
    "            \n",
    "            #self.y = tf.layers.dense(self.y[0], classes, activation = None)\n",
    "            \n",
    "            self.y = tf.slice(self.y[0], [0, 0], [-1,2])\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            self.regularized_loss = tf.losses.mean_squared_error(self.y_, self.y)\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:59:00.684124Z",
     "start_time": "2017-06-01T00:58:59.843181Z"
    }
   },
   "source": [
    "batch_iterations = 200\n",
    "\n",
    "x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.1)\n",
    "batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "for i in batch_indices:\n",
    "    print(x_train[i,np.newaxis,:])\n",
    "    print(y_train[i,np.newaxis,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:38:10.989038Z",
     "start_time": "2017-07-21T21:38:10.780376Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['key', 'no_of_features','hidden_layers','train_score', 'test_score', 'quality_score', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "\n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 1000\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_lstm_nsl_kdd-orig/hidden layers_{}_features count_{}\".format(h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            \n",
    "            for c, lr in enumerate(lrs):\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    for key, x_train, y_train in preprocess.get_data(train_paths):\n",
    "                        x_train, x_valid, y_train, y_valid, = ms.train_test_split(x_train, \n",
    "                                                                                  y_train, \n",
    "                                                                                  test_size=0.1)\n",
    "                        batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                                   batch_iterations)\n",
    "\n",
    "                        for i in batch_indices:\n",
    "\n",
    "                            _, train_loss = sess.run([net.train_op, net.regularized_loss], #net.summary_op\n",
    "                                                      feed_dict={net.x_input: x_train[i,np.newaxis,:], \n",
    "                                                                 net.y_input_: y_train[i,np.newaxis,:], \n",
    "                                                                 net.keep_prob:1, net.lr:lr})\n",
    "                            #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                            if(train_loss > 1e9):\n",
    "                                print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "\n",
    "\n",
    "                        valid_accuracy,valid_loss = sess.run([net.tf_accuracy, net.regularized_loss], #net.summary_op \n",
    "                                                              feed_dict={net.x_input: x_valid[:,np.newaxis,:], \n",
    "                                                                         net.y_input_: y_valid[:,np.newaxis,:], \n",
    "                                                                         net.keep_prob:1, net.lr:lr})\n",
    "                        #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "                        \n",
    "                    end_time = time.perf_counter() \n",
    "                    for key, x_test, y_test in preprocess.get_data(test_paths):\n",
    "                        accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                                               net.pred, \n",
    "                                                                               net.actual, net.y], \n",
    "                                                                              feed_dict={net.x_input: x_test[:,np.newaxis,:], \n",
    "                                                                                         net.y_input_: y_test[:,np.newaxis,:], \n",
    "                                                                                         net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "                        quality_score = me.matthews_corrcoef(actual_value, pred_value)\n",
    "                        recall = me.recall_score(actual_value, pred_value)\n",
    "                        prec = me.precision_score(actual_value, pred_value)\n",
    "                        accuracy = me.roc_auc_score(actual_value, pred_value)\n",
    "                        \n",
    "                        print(\"Key {} | Training Loss: {:.6f} | Train Accuracy: {:.6f} | Test Accuracy: {:.6f}, quality_score: {}, recall: {}, prec: {}\".format(key, train_loss, valid_accuracy, accuracy, quality_score, recall, prec))\n",
    "\n",
    "                        if accuracy > Train.best_acc_global:\n",
    "                                    Train.best_acc_global = accuracy\n",
    "                                    Train.pred_value = pred_value\n",
    "                                    Train.actual_value = actual_value\n",
    "\n",
    "                                    Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value, \"Actual\":actual_value})\n",
    "                        Train.predictions.update({\"{}_{}_{}\".format(key,f,h):\n",
    "                                                  (curr_pred, \n",
    "                                                   Train.result(key, f, h,valid_accuracy, accuracy, quality_score, end_time - start_time))})\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:38:11.049297Z",
     "start_time": "2017-07-21T21:38:10.990709Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "df_results = []\n",
    "past_scores = []\n",
    "\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "    def start_training():\n",
    "\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        \n",
    "        Train.predictions = {}\n",
    "        Train.results = []\n",
    "        \n",
    "        features_arr = [1] #[4, 8, 16, 32]\n",
    "        hidden_layers_arr = [1, 3]\n",
    "\n",
    "        epochs = [1]\n",
    "        lrs = [1e-2] #[1e-2, 1e-2/2, 1e-2/4]\n",
    "\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "            \n",
    "        dict1 = {}\n",
    "        dict2 = []\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "            \n",
    "        Train.predictions = dict1\n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "        #temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        if not os.path.isfile('dataset/scores/tf_lstm_nsl_kdd-orig_all.pkl'):\n",
    "            past_scores = df_results\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/scores/tf_lstm_nsl_kdd-orig_all.pkl\")\n",
    "\n",
    "        past_scores.append(df_results).to_pickle(\"dataset/scores/tf_lstm_nsl_kdd-orig_all.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:42:35.517858Z",
     "start_time": "2017-07-21T21:38:11.050801Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:1 hidden layers:1 features count:1\n",
      "Key 20151224 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.668208, quality_score: 0.24361090449302303, recall: 0.972579078569002, prec: 0.9887394508354209\n",
      "Key 20151204 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.670322, quality_score: 0.2537754215311179, recall: 0.9803740115331715, prec: 0.9913141439643025\n",
      "Key 20151216 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.680021, quality_score: 0.5022422043836925, recall: 0.996858291466295, prec: 0.9863768193066014\n",
      "Key 20151222 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.619577, quality_score: 0.1239461546796079, recall: 0.9518292615347104, prec: 0.9900398100185985\n",
      "Key 20151214 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.854176, quality_score: 0.839108427156579, recall: 0.999983033377588, prec: 0.9955701387657199\n",
      "Key 20151202 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.641244, quality_score: 0.4789518524148739, recall: 0.9974672549171375, prec: 0.9662390136210272\n",
      "Key 20151227 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.716301, quality_score: 0.5287305428598541, recall: 0.9972360773898331, prec: 0.9932829220484299\n",
      "Key 20151203 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.729154, quality_score: 0.5651518553313845, recall: 0.9835652139225863, prec: 0.9476491437844301\n",
      "Key 20151223 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.610060, quality_score: 0.2212749526241008, recall: 0.9732451402505516, prec: 0.9728666103733272\n",
      "Key 20151205 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.670604, quality_score: 0.24960163615267025, recall: 0.9790864082204578, prec: 0.9911711970706834\n",
      "Key 20151229 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.685552, quality_score: 0.5158532262660848, recall: 0.996202008879187, prec: 0.9827327190783993\n",
      "Key 20151208 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.667181, quality_score: 0.23497820797745028, recall: 0.9712388674306509, prec: 0.9890195619526847\n",
      "Key 20151219 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.592479, quality_score: 0.35114236607821, recall: 0.9940499616449321, prec: 0.7311894960477702\n",
      "Key 20151206 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.686712, quality_score: 0.23425091147164898, recall: 0.9671843429555759, prec: 0.9909149434473267\n",
      "Key 20151225 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.691197, quality_score: 0.28537643160079673, recall: 0.9796968674316116, prec: 0.99132238252826\n",
      "Key 20151210 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.823305, quality_score: 0.5957022476882496, recall: 0.9928869851369255, prec: 0.9953182053251007\n",
      "Key 20151217 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.752471, quality_score: 0.6080659228296393, recall: 0.9940478126524638, prec: 0.9831751648705164\n",
      "Key 20151207 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.606465, quality_score: 0.21423137801843, recall: 0.9722867249723302, prec: 0.9718332772963417\n",
      "Key 20151215 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.834341, quality_score: 0.8117838175185452, recall: 0.9998938168450422, prec: 0.9935602174245236\n",
      "Key 20151213 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.817837, quality_score: 0.7307889657435953, recall: 0.9988791313267783, prec: 0.9965603314861964\n",
      "Key 20151209 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.818156, quality_score: 0.7940438663129211, recall: 0.9999699533418323, prec: 0.9934880229944859\n",
      "Key 20151228 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.859887, quality_score: 0.7865805197008015, recall: 0.9986874798788189, prec: 0.9968141145710315\n",
      "Key 20151226 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.612387, quality_score: 0.45531442841983427, recall: 0.9999905513894182, prec: 0.9226977890405227\n",
      "Key 20151218 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.639448, quality_score: 0.17754756167674762, recall: 0.9518531044455847, prec: 0.984939813557788\n",
      "Key 20151231 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.769257, quality_score: 0.6023499442096111, recall: 0.9815563954728381, prec: 0.9648838799927135\n",
      "Key 20151212 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.805834, quality_score: 0.6820610098733783, recall: 0.9958778886007911, prec: 0.9914719304790868\n",
      "Key 20151211 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.858970, quality_score: 0.43988013822923416, recall: 0.9800280538394962, prec: 0.9973000634743233\n",
      "Key 20151221 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.622257, quality_score: 0.1022115915880505, recall: 0.9215451158408633, prec: 0.9902038479040018\n",
      "Key 20151201 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.694800, quality_score: 0.38201012043760435, recall: 0.9801515029030924, prec: 0.9814302515486694\n",
      "Key 20151220 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.690416, quality_score: 0.440699346805847, recall: 0.9782105118414233, prec: 0.9613859276161567\n",
      "Key 20151230 | Training Loss: 0.003847 | Train Accuracy: 0.999688 | Test Accuracy: 0.593210, quality_score: 0.3391403919382249, recall: 0.9946546999363439, prec: 0.9580836500578367\n",
      "Current Layer Attributes - epochs:1 hidden layers:3 features count:1\n",
      "Key 20151224 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151204 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151216 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151222 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151214 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 0.999962, quality_score: 0.9974691861165919, recall: 0.9999236501991458, prec: 1.0\n",
      "Key 20151202 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151227 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151203 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151223 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151205 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151229 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151208 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151219 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151206 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151225 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 0.999990, quality_score: 0.9992994732330477, recall: 0.9999801484893001, prec: 1.0\n",
      "Key 20151210 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151217 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 0.999928, quality_score: 0.9978774948961898, recall: 0.9998568872987478, prec: 1.0\n",
      "Key 20151207 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151215 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151213 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151209 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151228 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151226 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151218 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151231 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151212 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151211 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151221 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151201 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151220 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n",
      "Key 20151230 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, quality_score: 1.0, recall: 1.0, prec: 1.0\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -r 1\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T20:12:38.742950Z",
     "start_time": "2017-06-16T20:12:38.705898Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T20:12:38.748800Z",
     "start_time": "2017-06-16T20:12:38.744607Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:42:35.521963Z",
     "start_time": "2017-07-21T21:42:35.519361Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#g = df_results.groupby(by=['no_of_features'])\n",
    "#idx = g['test_score'].transform(max) == df_results['test_score']\n",
    "#df_results[idx].sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:42:35.591279Z",
     "start_time": "2017-07-21T21:42:35.523352Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:42:35.597769Z",
     "start_time": "2017-07-21T21:42:35.592808Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pd.Panel(Train.predictions).to_pickle(\"dataset/tf_lstm_nsl_kdd_predictions.pkl\")\n",
    "#df_results.to_pickle(\"dataset/tf_lstm_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:42:35.672268Z",
     "start_time": "2017-07-21T21:42:35.599230Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "    \n",
    "    #print(cm)\n",
    "\n",
    "    label = [[\"\\n True Positive\", \"\\n False Negative \\n Type II Error\"],\n",
    "             [\"\\n False Positive \\n Type I Error\", \"\\n True Negative\"]\n",
    "            ]\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        \n",
    "        plt.text(j, i, \"{} {}\".format(cm[i, j].round(4), label[i][j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, ['Normal', 'Attack'], normalize = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:42:35.685003Z",
     "start_time": "2017-07-21T21:42:35.673778Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "past_scores = pd.read_pickle(\"dataset/scores/tf_lstm_nsl_kdd-orig_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:42:35.730335Z",
     "start_time": "2017-07-21T21:42:35.686538Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>20151230</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>20151205</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>20151220</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>20151201</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>20151221</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>20151212</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>20151231</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>20151210</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>20151226</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>20151228</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>20151209</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>20151213</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>20151215</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>20151207</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>20151210</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>20151206</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>20151219</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>20151208</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>20151230</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>20151226</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>20151228</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>20151206</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>20151219</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>20151208</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>20151229</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>20151205</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>20151223</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20151203</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>20151202</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>20151209</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>20151222</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>20151216</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>20151204</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>20151224</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>20151207</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>20151215</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>20151213</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>20151229</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>20151218</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>20151223</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>20151231</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>20151202</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>20151220</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>20151201</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>20151221</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>20151222</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>20151216</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20151217</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.752471</td>\n",
       "      <td>0.608066</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20151217</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.752471</td>\n",
       "      <td>0.608066</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20151231</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.769257</td>\n",
       "      <td>0.602350</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20151231</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.769257</td>\n",
       "      <td>0.602350</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20151210</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.823305</td>\n",
       "      <td>0.595702</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20151210</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.823305</td>\n",
       "      <td>0.595702</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20151203</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.729154</td>\n",
       "      <td>0.565152</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20151203</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.729154</td>\n",
       "      <td>0.565152</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.716301</td>\n",
       "      <td>0.528731</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.716301</td>\n",
       "      <td>0.528731</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20151229</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.685552</td>\n",
       "      <td>0.515853</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20151229</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.685552</td>\n",
       "      <td>0.515853</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20151216</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.680021</td>\n",
       "      <td>0.502242</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20151216</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.680021</td>\n",
       "      <td>0.502242</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20151202</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.641244</td>\n",
       "      <td>0.478952</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20151202</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.641244</td>\n",
       "      <td>0.478952</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20151226</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.612387</td>\n",
       "      <td>0.455314</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20151226</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.612387</td>\n",
       "      <td>0.455314</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20151220</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.690416</td>\n",
       "      <td>0.440699</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20151220</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.690416</td>\n",
       "      <td>0.440699</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.858970</td>\n",
       "      <td>0.439880</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.858970</td>\n",
       "      <td>0.439880</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20151201</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.694800</td>\n",
       "      <td>0.382010</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20151201</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.694800</td>\n",
       "      <td>0.382010</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20151219</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.592479</td>\n",
       "      <td>0.351142</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20151219</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.592479</td>\n",
       "      <td>0.351142</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20151230</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.593210</td>\n",
       "      <td>0.339140</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20151230</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.593210</td>\n",
       "      <td>0.339140</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20151225</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.691197</td>\n",
       "      <td>0.285376</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20151225</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.691197</td>\n",
       "      <td>0.285376</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20151204</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.670322</td>\n",
       "      <td>0.253775</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20151204</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.670322</td>\n",
       "      <td>0.253775</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20151205</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.670604</td>\n",
       "      <td>0.249602</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20151205</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.670604</td>\n",
       "      <td>0.249602</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20151224</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.668208</td>\n",
       "      <td>0.243611</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20151224</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.668208</td>\n",
       "      <td>0.243611</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20151208</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.667181</td>\n",
       "      <td>0.234978</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20151208</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.667181</td>\n",
       "      <td>0.234978</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20151206</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.686712</td>\n",
       "      <td>0.234251</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20151206</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.686712</td>\n",
       "      <td>0.234251</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20151223</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.610060</td>\n",
       "      <td>0.221275</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20151223</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.610060</td>\n",
       "      <td>0.221275</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20151207</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.606465</td>\n",
       "      <td>0.214231</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20151207</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.606465</td>\n",
       "      <td>0.214231</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20151218</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.639448</td>\n",
       "      <td>0.177548</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20151218</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.639448</td>\n",
       "      <td>0.177548</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20151222</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.619577</td>\n",
       "      <td>0.123946</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20151222</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.619577</td>\n",
       "      <td>0.123946</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20151221</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.622257</td>\n",
       "      <td>0.102212</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20151221</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.622257</td>\n",
       "      <td>0.102212</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         key  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "61  20151230               1              3     1.000000    1.000000   \n",
       "40  20151205               1              3     1.000000    1.000000   \n",
       "60  20151220               1              3     1.000000    1.000000   \n",
       "59  20151201               1              3     1.000000    1.000000   \n",
       "58  20151221               1              3     1.000000    1.000000   \n",
       "57  20151211               1              3     1.000000    1.000000   \n",
       "56  20151212               1              3     1.000000    1.000000   \n",
       "55  20151231               1              3     1.000000    1.000000   \n",
       "46  20151210               1              3     1.000000    1.000000   \n",
       "53  20151226               1              3     1.000000    1.000000   \n",
       "52  20151228               1              3     1.000000    1.000000   \n",
       "51  20151209               1              3     1.000000    1.000000   \n",
       "50  20151213               1              3     1.000000    1.000000   \n",
       "49  20151215               1              3     1.000000    1.000000   \n",
       "48  20151207               1              3     1.000000    1.000000   \n",
       "46  20151210               1              3     1.000000    1.000000   \n",
       "44  20151206               1              3     1.000000    1.000000   \n",
       "43  20151219               1              3     1.000000    1.000000   \n",
       "42  20151208               1              3     1.000000    1.000000   \n",
       "61  20151230               1              3     1.000000    1.000000   \n",
       "53  20151226               1              3     1.000000    1.000000   \n",
       "52  20151228               1              3     1.000000    1.000000   \n",
       "37  20151227               1              3     1.000000    1.000000   \n",
       "44  20151206               1              3     1.000000    1.000000   \n",
       "43  20151219               1              3     1.000000    1.000000   \n",
       "42  20151208               1              3     1.000000    1.000000   \n",
       "41  20151229               1              3     1.000000    1.000000   \n",
       "40  20151205               1              3     1.000000    1.000000   \n",
       "39  20151223               1              3     1.000000    1.000000   \n",
       "38  20151203               1              3     1.000000    1.000000   \n",
       "36  20151202               1              3     1.000000    1.000000   \n",
       "51  20151209               1              3     1.000000    1.000000   \n",
       "34  20151222               1              3     1.000000    1.000000   \n",
       "33  20151216               1              3     1.000000    1.000000   \n",
       "32  20151204               1              3     1.000000    1.000000   \n",
       "31  20151224               1              3     1.000000    1.000000   \n",
       "48  20151207               1              3     1.000000    1.000000   \n",
       "49  20151215               1              3     1.000000    1.000000   \n",
       "50  20151213               1              3     1.000000    1.000000   \n",
       "41  20151229               1              3     1.000000    1.000000   \n",
       "54  20151218               1              3     1.000000    1.000000   \n",
       "39  20151223               1              3     1.000000    1.000000   \n",
       "55  20151231               1              3     1.000000    1.000000   \n",
       "37  20151227               1              3     1.000000    1.000000   \n",
       "36  20151202               1              3     1.000000    1.000000   \n",
       "60  20151220               1              3     1.000000    1.000000   \n",
       "59  20151201               1              3     1.000000    1.000000   \n",
       "58  20151221               1              3     1.000000    1.000000   \n",
       "34  20151222               1              3     1.000000    1.000000   \n",
       "33  20151216               1              3     1.000000    1.000000   \n",
       "..       ...             ...            ...          ...         ...   \n",
       "16  20151217               1              1     0.999688    0.752471   \n",
       "16  20151217               1              1     0.999688    0.752471   \n",
       "24  20151231               1              1     0.999688    0.769257   \n",
       "24  20151231               1              1     0.999688    0.769257   \n",
       "15  20151210               1              1     0.999688    0.823305   \n",
       "15  20151210               1              1     0.999688    0.823305   \n",
       "7   20151203               1              1     0.999688    0.729154   \n",
       "7   20151203               1              1     0.999688    0.729154   \n",
       "6   20151227               1              1     0.999688    0.716301   \n",
       "6   20151227               1              1     0.999688    0.716301   \n",
       "10  20151229               1              1     0.999688    0.685552   \n",
       "10  20151229               1              1     0.999688    0.685552   \n",
       "2   20151216               1              1     0.999688    0.680021   \n",
       "2   20151216               1              1     0.999688    0.680021   \n",
       "5   20151202               1              1     0.999688    0.641244   \n",
       "5   20151202               1              1     0.999688    0.641244   \n",
       "22  20151226               1              1     0.999688    0.612387   \n",
       "22  20151226               1              1     0.999688    0.612387   \n",
       "29  20151220               1              1     0.999688    0.690416   \n",
       "29  20151220               1              1     0.999688    0.690416   \n",
       "26  20151211               1              1     0.999688    0.858970   \n",
       "26  20151211               1              1     0.999688    0.858970   \n",
       "28  20151201               1              1     0.999688    0.694800   \n",
       "28  20151201               1              1     0.999688    0.694800   \n",
       "12  20151219               1              1     0.999688    0.592479   \n",
       "12  20151219               1              1     0.999688    0.592479   \n",
       "30  20151230               1              1     0.999688    0.593210   \n",
       "30  20151230               1              1     0.999688    0.593210   \n",
       "14  20151225               1              1     0.999688    0.691197   \n",
       "14  20151225               1              1     0.999688    0.691197   \n",
       "1   20151204               1              1     0.999688    0.670322   \n",
       "1   20151204               1              1     0.999688    0.670322   \n",
       "9   20151205               1              1     0.999688    0.670604   \n",
       "9   20151205               1              1     0.999688    0.670604   \n",
       "0   20151224               1              1     0.999688    0.668208   \n",
       "0   20151224               1              1     0.999688    0.668208   \n",
       "11  20151208               1              1     0.999688    0.667181   \n",
       "11  20151208               1              1     0.999688    0.667181   \n",
       "13  20151206               1              1     0.999688    0.686712   \n",
       "13  20151206               1              1     0.999688    0.686712   \n",
       "8   20151223               1              1     0.999688    0.610060   \n",
       "8   20151223               1              1     0.999688    0.610060   \n",
       "17  20151207               1              1     0.999688    0.606465   \n",
       "17  20151207               1              1     0.999688    0.606465   \n",
       "23  20151218               1              1     0.999688    0.639448   \n",
       "23  20151218               1              1     0.999688    0.639448   \n",
       "3   20151222               1              1     0.999688    0.619577   \n",
       "3   20151222               1              1     0.999688    0.619577   \n",
       "27  20151221               1              1     0.999688    0.622257   \n",
       "27  20151221               1              1     0.999688    0.622257   \n",
       "\n",
       "    quality_score  time_taken  \n",
       "61       1.000000    9.201964  \n",
       "40       1.000000    9.201964  \n",
       "60       1.000000    9.201964  \n",
       "59       1.000000    9.201964  \n",
       "58       1.000000    9.201964  \n",
       "57       1.000000    9.201964  \n",
       "56       1.000000    9.201964  \n",
       "55       1.000000    9.201964  \n",
       "46       1.000000    9.201964  \n",
       "53       1.000000    9.201964  \n",
       "52       1.000000    9.201964  \n",
       "51       1.000000    9.201964  \n",
       "50       1.000000    9.201964  \n",
       "49       1.000000    9.201964  \n",
       "48       1.000000    9.201964  \n",
       "46       1.000000    9.201964  \n",
       "44       1.000000    9.201964  \n",
       "43       1.000000    9.201964  \n",
       "42       1.000000    9.201964  \n",
       "61       1.000000    9.201964  \n",
       "53       1.000000    9.201964  \n",
       "52       1.000000    9.201964  \n",
       "37       1.000000    9.201964  \n",
       "44       1.000000    9.201964  \n",
       "43       1.000000    9.201964  \n",
       "42       1.000000    9.201964  \n",
       "41       1.000000    9.201964  \n",
       "40       1.000000    9.201964  \n",
       "39       1.000000    9.201964  \n",
       "38       1.000000    9.201964  \n",
       "36       1.000000    9.201964  \n",
       "51       1.000000    9.201964  \n",
       "34       1.000000    9.201964  \n",
       "33       1.000000    9.201964  \n",
       "32       1.000000    9.201964  \n",
       "31       1.000000    9.201964  \n",
       "48       1.000000    9.201964  \n",
       "49       1.000000    9.201964  \n",
       "50       1.000000    9.201964  \n",
       "41       1.000000    9.201964  \n",
       "54       1.000000    9.201964  \n",
       "39       1.000000    9.201964  \n",
       "55       1.000000    9.201964  \n",
       "37       1.000000    9.201964  \n",
       "36       1.000000    9.201964  \n",
       "60       1.000000    9.201964  \n",
       "59       1.000000    9.201964  \n",
       "58       1.000000    9.201964  \n",
       "34       1.000000    9.201964  \n",
       "33       1.000000    9.201964  \n",
       "..            ...         ...  \n",
       "16       0.608066    6.222824  \n",
       "16       0.608066    6.222824  \n",
       "24       0.602350    6.222824  \n",
       "24       0.602350    6.222824  \n",
       "15       0.595702    6.222824  \n",
       "15       0.595702    6.222824  \n",
       "7        0.565152    6.222824  \n",
       "7        0.565152    6.222824  \n",
       "6        0.528731    6.222824  \n",
       "6        0.528731    6.222824  \n",
       "10       0.515853    6.222824  \n",
       "10       0.515853    6.222824  \n",
       "2        0.502242    6.222824  \n",
       "2        0.502242    6.222824  \n",
       "5        0.478952    6.222824  \n",
       "5        0.478952    6.222824  \n",
       "22       0.455314    6.222824  \n",
       "22       0.455314    6.222824  \n",
       "29       0.440699    6.222824  \n",
       "29       0.440699    6.222824  \n",
       "26       0.439880    6.222824  \n",
       "26       0.439880    6.222824  \n",
       "28       0.382010    6.222824  \n",
       "28       0.382010    6.222824  \n",
       "12       0.351142    6.222824  \n",
       "12       0.351142    6.222824  \n",
       "30       0.339140    6.222824  \n",
       "30       0.339140    6.222824  \n",
       "14       0.285376    6.222824  \n",
       "14       0.285376    6.222824  \n",
       "1        0.253775    6.222824  \n",
       "1        0.253775    6.222824  \n",
       "9        0.249602    6.222824  \n",
       "9        0.249602    6.222824  \n",
       "0        0.243611    6.222824  \n",
       "0        0.243611    6.222824  \n",
       "11       0.234978    6.222824  \n",
       "11       0.234978    6.222824  \n",
       "13       0.234251    6.222824  \n",
       "13       0.234251    6.222824  \n",
       "8        0.221275    6.222824  \n",
       "8        0.221275    6.222824  \n",
       "17       0.214231    6.222824  \n",
       "17       0.214231    6.222824  \n",
       "23       0.177548    6.222824  \n",
       "23       0.177548    6.222824  \n",
       "3        0.123946    6.222824  \n",
       "3        0.123946    6.222824  \n",
       "27       0.102212    6.222824  \n",
       "27       0.102212    6.222824  \n",
       "\n",
       "[124 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores.sort_values(by='quality_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:42:35.749676Z",
     "start_time": "2017-07-21T21:42:35.731776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>3</th>\n",
       "      <td>20151230</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20151214</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.854176</td>\n",
       "      <td>0.839108</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   key  train_score  test_score  \\\n",
       "no_of_features hidden_layers                                      \n",
       "1              3              20151230     1.000000    1.000000   \n",
       "               1              20151214     0.999688    0.854176   \n",
       "\n",
       "                              quality_score  time_taken  \n",
       "no_of_features hidden_layers                             \n",
       "1              3                   1.000000    9.201964  \n",
       "               1                   0.839108    6.222824  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.sort_values(by='quality_score', ascending=False).groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.first().sort_values(by='quality_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:42:42.994447Z",
     "start_time": "2017-07-21T21:42:35.751246Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#key_nof_hidden '20151201_16_1'\n",
    "Train.predictions = pd.read_pickle(\"dataset/tf_lstm_nsl_kdd_predictions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:42:43.019251Z",
     "start_time": "2017-07-21T21:42:42.996701Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = Train.predictions['20151228_1_3'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:42:43.028184Z",
     "start_time": "2017-07-21T21:42:43.021193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train.predictions['20151219_42_1'].loc[:,'Prediction']\n",
    "df.loc[:,'Prediction'].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:42:43.084852Z",
     "start_time": "2017-07-21T21:42:43.030367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics as me\n",
    "me.f1_score(df.loc[:,'Actual'].values.astype(int),\n",
    "            df.loc[:,'Prediction'].values.astype(int) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:42:43.100797Z",
     "start_time": "2017-07-21T21:42:43.086505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0      4159\n",
       "1.0    363423\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:42:43.911629Z",
     "start_time": "2017-07-21T21:42:43.102554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGkCAYAAACy1WveAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8ba4f85400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:42:43.924696Z",
     "start_time": "2017-07-21T21:42:43.913082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999827</td>\n",
       "      <td>9.201964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.444852</td>\n",
       "      <td>6.222824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              train_score  test_score  quality_score  \\\n",
       "no_of_features hidden_layers                                           \n",
       "1              3                 1.000000    0.999996       0.999827   \n",
       "               1                 0.999688    0.709091       0.444852   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "1              3                9.201964  \n",
       "               1                6.222824  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.mean().sort_values(by='quality_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:42:43.982632Z",
     "start_time": "2017-07-21T21:42:43.926188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085877</td>\n",
       "      <td>0.215775</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              train_score  test_score  quality_score  \\\n",
       "no_of_features hidden_layers                                           \n",
       "1              1                      0.0    0.085877       0.215775   \n",
       "               3                      0.0    0.000014       0.000586   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "1              1                     0.0  \n",
       "               3                     0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:42:43.998277Z",
     "start_time": "2017-07-21T21:42:43.984071Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_of_features  hidden_layers\n",
       "1               1                (0.021939629542, 0.867764005685)\n",
       "                3                 (0.998678615375, 1.00097597522)\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fn(x):\n",
    "    #print(x)\n",
    "    return stats.norm.interval(0.95, loc=x.quality_score.mean(), scale=x.quality_score.std())\n",
    "psg.apply(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

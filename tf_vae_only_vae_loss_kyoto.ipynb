{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:35:39.684857Z",
     "start_time": "2017-07-21T22:35:33.707392Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:35:39.891289Z",
     "start_time": "2017-07-21T22:35:39.686438Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm dataset/scores/tf_vae_only_vae_loss_nsl_kdd_scores_all.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:35:40.083008Z",
     "start_time": "2017-07-21T22:35:39.893162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20140115': {'x': 'dataset/Kyoto2016/2014/01/20140115_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140115_y.csv'}, '20140107': {'y': 'dataset/Kyoto2016/2014/01/20140107_y.csv', 'x': 'dataset/Kyoto2016/2014/01/20140107_x.csv'}, '20140109': {'x': 'dataset/Kyoto2016/2014/01/20140109_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140109_y.csv'}, '20140104': {'y': 'dataset/Kyoto2016/2014/01/20140104_y.csv', 'x': 'dataset/Kyoto2016/2014/01/20140104_x.csv'}, '20140119': {'x': 'dataset/Kyoto2016/2014/01/20140119_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140119_y.csv'}, '20140120': {'x': 'dataset/Kyoto2016/2014/01/20140120_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140120_y.csv'}, '20140123': {'x': 'dataset/Kyoto2016/2014/01/20140123_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140123_y.csv'}}\n",
      "----------------------------------------------------------------------------------------\n",
      "{'20151224': {'y': 'dataset/Kyoto2016/2015/12/20151224_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151224_x.csv'}, '20151204': {'y': 'dataset/Kyoto2016/2015/12/20151204_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151204_x.csv'}, '20151216': {'x': 'dataset/Kyoto2016/2015/12/20151216_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151216_y.csv'}, '20151222': {'x': 'dataset/Kyoto2016/2015/12/20151222_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151222_y.csv'}, '20151214': {'x': 'dataset/Kyoto2016/2015/12/20151214_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151214_y.csv'}, '20151202': {'x': 'dataset/Kyoto2016/2015/12/20151202_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151202_y.csv'}, '20151227': {'y': 'dataset/Kyoto2016/2015/12/20151227_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151227_x.csv'}, '20151203': {'y': 'dataset/Kyoto2016/2015/12/20151203_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151203_x.csv'}, '20151223': {'y': 'dataset/Kyoto2016/2015/12/20151223_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151223_x.csv'}, '20151205': {'x': 'dataset/Kyoto2016/2015/12/20151205_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151205_y.csv'}, '20151229': {'x': 'dataset/Kyoto2016/2015/12/20151229_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151229_y.csv'}, '20151208': {'x': 'dataset/Kyoto2016/2015/12/20151208_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151208_y.csv'}, '20151219': {'y': 'dataset/Kyoto2016/2015/12/20151219_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151219_x.csv'}, '20151206': {'y': 'dataset/Kyoto2016/2015/12/20151206_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151206_x.csv'}, '20151225': {'x': 'dataset/Kyoto2016/2015/12/20151225_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151225_y.csv'}, '20151210': {'x': 'dataset/Kyoto2016/2015/12/20151210_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151210_y.csv'}, '20151217': {'x': 'dataset/Kyoto2016/2015/12/20151217_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151217_y.csv'}, '20151207': {'x': 'dataset/Kyoto2016/2015/12/20151207_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151207_y.csv'}, '20151215': {'x': 'dataset/Kyoto2016/2015/12/20151215_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151215_y.csv'}, '20151213': {'y': 'dataset/Kyoto2016/2015/12/20151213_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151213_x.csv'}, '20151209': {'x': 'dataset/Kyoto2016/2015/12/20151209_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151209_y.csv'}, '20151228': {'y': 'dataset/Kyoto2016/2015/12/20151228_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151228_x.csv'}, '20151226': {'y': 'dataset/Kyoto2016/2015/12/20151226_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151226_x.csv'}, '20151218': {'x': 'dataset/Kyoto2016/2015/12/20151218_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151218_y.csv'}, '20151231': {'x': 'dataset/Kyoto2016/2015/12/20151231_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151231_y.csv'}, '20151212': {'x': 'dataset/Kyoto2016/2015/12/20151212_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151212_y.csv'}, '20151211': {'x': 'dataset/Kyoto2016/2015/12/20151211_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151211_y.csv'}, '20151221': {'y': 'dataset/Kyoto2016/2015/12/20151221_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151221_x.csv'}, '20151201': {'x': 'dataset/Kyoto2016/2015/12/20151201_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151201_y.csv'}, '20151220': {'x': 'dataset/Kyoto2016/2015/12/20151220_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151220_y.csv'}, '20151230': {'y': 'dataset/Kyoto2016/2015/12/20151230_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151230_x.csv'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class preprocess:\n",
    "    \n",
    "    paths = {}\n",
    "\n",
    "    def get_files(folder_path):\n",
    "        paths = {}\n",
    "        for path, subdirs, files in os.walk(folder_path):\n",
    "            for name in files:\n",
    "                if name.endswith(\"csv\"):\n",
    "                    key = name.split(\"_\")[0]\n",
    "\n",
    "                    if paths.get(key) is None:\n",
    "                        paths[key] = {}\n",
    "\n",
    "                    if name.endswith(\"_x.csv\"):\n",
    "                        x = os.path.join(path, name)\n",
    "                        paths[key]['x'] = x\n",
    "                    elif name.endswith(\"_y.csv\"):\n",
    "                        y = os.path.join(path, name)\n",
    "                        paths[key]['y'] = y\n",
    "        preprocess.paths = paths\n",
    "        return paths\n",
    "\n",
    "    def get_data(paths):\n",
    "        for key, value in paths.items():\n",
    "            x = pd.read_csv(value['x'])\n",
    "            y = pd.read_csv(value['y'])\n",
    "            #print(x.shape)\n",
    "            #print(x.values.shape)\n",
    "            #print(y.sum())\n",
    "            yield key, x.values, y.values\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "train_paths = preprocess.get_files(\"dataset/Kyoto2016/2014/01\")\n",
    "test_paths = preprocess.get_files(\"dataset/Kyoto2016/2015/12\")\n",
    "#test_paths.update(preprocess.get_files(\"dataset/Kyoto2016/2015/11\"))\n",
    "#test_paths.update(preprocess.get_files(\"dataset/Kyoto2016/2015/10\"))\n",
    "\n",
    "paths = {}\n",
    "keys = train_paths.keys()\n",
    "for key in list(keys)[0:7]:\n",
    "    paths.update({key: train_paths[key]})\n",
    "train_paths = paths\n",
    "\n",
    "print(train_paths)\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "#test_paths = test_paths.popitem()\n",
    "#test_paths = {test_paths[0]: test_paths[1]}\n",
    "print(test_paths)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:35:50.771766Z",
     "start_time": "2017-07-21T22:35:40.084532Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import metrics as me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:35:51.198533Z",
     "start_time": "2017-07-21T22:35:50.773551Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 42\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 42\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 42\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            self.x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "            self.y = tf.slice(self.x_hat, [0,input_dim-2], [-1,-1])\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            loss = tf.reduce_mean((BCE + KLD + softmax_loss) * lam)\n",
    "\n",
    "            #loss = tf.clip_by_value(loss, -1e-2, 1e-2)\n",
    "            #loss = tf.where(tf.is_nan(loss), 1e-2, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-2), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-2), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = tf.abs(loss, name = \"Regularized_loss\")\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:35:51.447823Z",
     "start_time": "2017-07-21T22:35:51.200286Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['key', 'no_of_features','hidden_layers','train_score', 'test_score', 'quality_score', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "\n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_vae_only_vae_loss_nsl_kdd/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            for lr in lrs:\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    #print(\"Step {} | Training Loss:\".format(epoch), end = \" \" )\n",
    "                    for key, x_train, y_train in preprocess.get_data(train_paths):\n",
    "                        x_train, x_valid, y_train, y_valid, = ms.train_test_split(x_train, \n",
    "                                                                                  y_train, \n",
    "                                                                                  test_size=0.1)\n",
    "                        batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                                   batch_iterations)\n",
    "\n",
    "                        for i in batch_indices:\n",
    "\n",
    "                            def train_batch():\n",
    "                                nonlocal train_loss\n",
    "                                _, train_loss = sess.run([net.train_op, \n",
    "                                                           net.regularized_loss, \n",
    "                                                           ], #net.summary_op\n",
    "                                                          feed_dict={net.x: x_train[i,:], \n",
    "                                                                     net.y_: y_train[i,:], \n",
    "                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                            train_batch()\n",
    "\n",
    "                            count = 10\n",
    "                            while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1 and count > 1):\n",
    "                                print(\"Step {} | High Training Loss: {:.6f} ... Restoring Net\".format(epoch, train_loss))\n",
    "                                net.saver.restore(sess, \n",
    "                                                  tf.train.latest_checkpoint('dataset/tf_vae_only_vae_loss_nsl_kdd/hidden layers_{}_features count_{}'\n",
    "                                                                             .format(epochs,h,f)))\n",
    "                                train_batch()\n",
    "                                count -=1\n",
    "\n",
    "                            #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                            #if(train_loss > 1e9):\n",
    "\n",
    "                            #print(\"{:.6f}\".format(train_loss), end = \", \" )\n",
    "\n",
    "                        #print(\"\")\n",
    "                        valid_loss, valid_accuracy = sess.run([net.regularized_loss, net.tf_accuracy], feed_dict={net.x: x_valid, \n",
    "                                                                             net.y_: y_valid, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    end_time = time.perf_counter()\n",
    "                    for key, x_test, y_test in preprocess.get_data(test_paths):\n",
    "\n",
    "                        accuracy, test_loss, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, net.regularized_loss, \n",
    "                                                                       net.pred, \n",
    "                                                                       net.actual, net.y], \n",
    "                                                                      feed_dict={net.x: x_test, \n",
    "                                                                                 net.y_: y_test, \n",
    "                                                                                 net.keep_prob:1, net.lr:lr})\n",
    "                        \n",
    "                        quality_score = me.matthews_corrcoef(actual_value, pred_value)\n",
    "                        recall = me.recall_score(actual_value, pred_value)\n",
    "                        prec = me.precision_score(actual_value, pred_value)\n",
    "                        accuracy = me.roc_auc_score(actual_value, pred_value)\n",
    "                        \n",
    "                        print(\"Key {} | Training Loss: {:.6f} | Train Accuracy: {:.6f} | Test Accuracy: {:.6f}, quality_score: {}\".format(key, train_loss, valid_accuracy, accuracy, quality_score))\n",
    "                       \n",
    "\n",
    "                        if accuracy > Train.best_acc_global:\n",
    "                            Train.best_acc_global = accuracy\n",
    "\n",
    "                            Train.pred_value = pred_value\n",
    "                            Train.actual_value = actual_value\n",
    "\n",
    "                            Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value, \"Actual\":actual_value})\n",
    "                        Train.predictions.update({\"{}_{}_{}\".format(key,f,h):\n",
    "                                                  (curr_pred, \n",
    "                                                   Train.result(key, f, h,valid_accuracy, accuracy, quality_score, end_time - start_time))})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:35:51.513968Z",
     "start_time": "2017-07-21T22:35:51.449429Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "df_results = []\n",
    "past_scores = []\n",
    "\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "    def start_training():\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        print(\"********************************** Training ******************************\")\n",
    "        Train.predictions = {}\n",
    "        Train.results = []\n",
    "    \n",
    "        \n",
    "        features_arr = [1, 4, 8, 16, 42]\n",
    "        hidden_layers_arr = [1, 3]\n",
    "\n",
    "        epochs = [1]\n",
    "        lrs = [1e-5]\n",
    "        print(\"***************************** Entering Loop **********************\")\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - hidden layers:{} features count:{}\".format(h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "            \n",
    "        dict1 = {}\n",
    "        dict2 = []\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "        Train.predictions = dict1\n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "\n",
    "        #temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        if not os.path.isfile('dataset/scores/tf_vae_only_vae_loss_nsl_kdd_scores_all.pkl'):\n",
    "            past_scores = df_results#temp\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/scores/tf_vae_only_vae_loss_nsl_kdd_scores_all.pkl\")\n",
    "\n",
    "        past_scores.append(df_results, ignore_index=True).to_pickle(\"dataset/scores/tf_vae_only_vae_loss_nsl_kdd_scores_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:55:26.722662Z",
     "start_time": "2017-07-21T22:35:51.515626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************** Training ******************************\n",
      "***************************** Entering Loop **********************\n",
      "Current Layer Attributes - hidden layers:1 features count:1\n",
      "Key 20151224 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.495731, quality_score: -0.002561397883699275\n",
      "Key 20151204 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.491857, quality_score: -0.004181009557801292\n",
      "Key 20151216 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.457382, quality_score: -0.02759923482715811\n",
      "Key 20151222 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.434051, quality_score: -0.03327870705543093\n",
      "Key 20151214 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.467155, quality_score: -0.017923957347894873\n",
      "Key 20151202 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.505657, quality_score: 0.00535877127260692\n",
      "Key 20151227 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.472610, quality_score: -0.013140311206469364\n",
      "Key 20151203 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.538244, quality_score: 0.0505180007548656\n",
      "Key 20151223 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.485457, quality_score: -0.01217644755168092\n",
      "Key 20151205 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.460839, quality_score: -0.020162586761729957\n",
      "Key 20151229 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.450044, quality_score: -0.03528206647592674\n",
      "Key 20151208 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.494397, quality_score: -0.0033159838674764824\n",
      "Key 20151219 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.516278, quality_score: 0.032117477047290094\n",
      "Key 20151206 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.482910, quality_score: -0.009425711850830257\n",
      "Key 20151225 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.509543, quality_score: 0.005301887241131081\n",
      "Key 20151210 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.468123, quality_score: -0.016344866121067378\n",
      "Key 20151217 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.541014, quality_score: 0.032794882181547004\n",
      "Key 20151207 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.490933, quality_score: -0.007601475765533914\n",
      "Key 20151215 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.466848, quality_score: -0.020407627322512858\n",
      "Key 20151213 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.476627, quality_score: -0.009817127795108314\n",
      "Key 20151209 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.466677, quality_score: -0.01967191139603092\n",
      "Key 20151228 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.465470, quality_score: -0.01568635253179519\n",
      "Key 20151226 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.446643, quality_score: -0.06747872060564465\n",
      "Key 20151218 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.477538, quality_score: -0.014804637357413514\n",
      "Key 20151231 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.459806, quality_score: -0.04679365384547064\n",
      "Key 20151212 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.470929, quality_score: -0.018266605579263132\n",
      "Key 20151211 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.445132, quality_score: -0.023734569889525724\n",
      "Key 20151221 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.457269, quality_score: -0.02180039009115066\n",
      "Key 20151201 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.571151, quality_score: 0.05636367086014196\n",
      "Key 20151220 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.492086, quality_score: -0.008321845766557577\n",
      "Key 20151230 | Training Loss: 0.020481 | Train Accuracy: 0.751854 | Test Accuracy: 0.506670, quality_score: 0.0067487101758108795\n",
      "Current Layer Attributes - hidden layers:1 features count:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:516: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(var_yt * var_yp)\n",
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151224 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151204 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151216 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151222 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151214 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151202 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151227 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151203 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151223 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151205 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151229 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151208 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151219 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151206 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151225 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151210 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151217 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151207 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151215 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151213 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151209 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151228 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151226 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151218 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151231 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151212 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151211 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151221 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151201 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151220 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151230 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Current Layer Attributes - hidden layers:1 features count:8\n",
      "Key 20151224 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.510864, quality_score: 0.005676084578117787\n",
      "Key 20151204 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.513659, quality_score: 0.006286759453288867\n",
      "Key 20151216 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.468256, quality_score: -0.018438055291066447\n",
      "Key 20151222 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.492089, quality_score: -0.003668491915069859\n",
      "Key 20151214 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.488524, quality_score: -0.005611649183871947\n",
      "Key 20151202 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.509386, quality_score: 0.007969331887959397\n",
      "Key 20151227 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.521707, quality_score: 0.00946180160675333\n",
      "Key 20151203 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.464632, quality_score: -0.0415526477474854\n",
      "Key 20151223 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.527746, quality_score: 0.020463390336485133\n",
      "Key 20151205 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.533732, quality_score: 0.015650696581651007\n",
      "Key 20151229 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.546709, quality_score: 0.030712739177889126\n",
      "Key 20151208 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.485417, quality_score: -0.007510219773292676\n",
      "Key 20151219 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.442603, quality_score: -0.10714727334224315\n",
      "Key 20151206 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.517531, quality_score: 0.008505440306418496\n",
      "Key 20151225 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.494844, quality_score: -0.0024517580854952693\n",
      "Key 20151210 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.494780, quality_score: -0.0024066461168421238\n",
      "Key 20151217 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.445731, quality_score: -0.03935932609625808\n",
      "Key 20151207 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.539209, quality_score: 0.029312455273652675\n",
      "Key 20151215 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.472300, quality_score: -0.015266870817294099\n",
      "Key 20151213 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.532507, quality_score: 0.01266371791170196\n",
      "Key 20151209 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.522892, quality_score: 0.012171188900938223\n",
      "Key 20151228 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.545451, quality_score: 0.019452060500042473\n",
      "Key 20151226 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.526129, quality_score: 0.03130822975635596\n",
      "Key 20151218 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.487690, quality_score: -0.007121480481793166\n",
      "Key 20151231 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.524822, quality_score: 0.02628981891157493\n",
      "Key 20151212 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.520431, quality_score: 0.012058356018641027\n",
      "Key 20151211 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.524465, quality_score: 0.009840086559935148\n",
      "Key 20151221 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.471578, quality_score: -0.013159132452489961\n",
      "Key 20151201 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.496334, quality_score: -0.0025425320005852538\n",
      "Key 20151220 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.489158, quality_score: -0.010536578008359745\n",
      "Key 20151230 | Training Loss: 5.046694 | Train Accuracy: 0.533915 | Test Accuracy: 0.480834, quality_score: -0.016981909375103015\n",
      "Current Layer Attributes - hidden layers:1 features count:16\n",
      "Key 20151224 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151204 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151216 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151222 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151214 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151202 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151227 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151203 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151223 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151205 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151229 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151208 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151219 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151206 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151225 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151210 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151217 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151207 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151215 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151213 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151209 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151228 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151226 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151218 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151231 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151212 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151211 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151221 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151201 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151220 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151230 | Training Loss: nan | Train Accuracy: 0.003517 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Current Layer Attributes - hidden layers:1 features count:42\n",
      "Key 20151224 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151204 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151216 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151222 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151214 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151202 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151227 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151203 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151223 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151205 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151229 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151208 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151219 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151206 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151225 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151210 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151217 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151207 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151215 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151213 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151209 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151228 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151226 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151218 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151231 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151212 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151211 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151221 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151201 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151220 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151230 | Training Loss: nan | Train Accuracy: 0.003690 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Current Layer Attributes - hidden layers:3 features count:1\n",
      "Key 20151224 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151204 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151216 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151222 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151214 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151202 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151227 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151203 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151223 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151205 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151229 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151208 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151219 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151206 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151225 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151210 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151217 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151207 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151215 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151213 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151209 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151228 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151226 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151218 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151231 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151212 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151211 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151221 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151201 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151220 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Key 20151230 | Training Loss: 0.012645 | Train Accuracy: 0.995758 | Test Accuracy: 0.500000, quality_score: 0.0\n",
      "Current Layer Attributes - hidden layers:3 features count:4\n",
      "Key 20151224 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.500440, quality_score: 0.001189689109211047\n",
      "Key 20151204 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.499522, quality_score: -0.0011370188892996257\n",
      "Key 20151216 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.503498, quality_score: 0.010843004573030194\n",
      "Key 20151222 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.498247, quality_score: -0.00446375825496952\n",
      "Key 20151214 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.500295, quality_score: 0.0007533318018755832\n",
      "Key 20151202 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.502684, quality_score: 0.011854494394103788\n",
      "Key 20151227 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.497914, quality_score: -0.0044238669941314935\n",
      "Key 20151203 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.504681, quality_score: 0.028897077796651627\n",
      "Key 20151223 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.498782, quality_score: -0.004580195272727354\n",
      "Key 20151205 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.497747, quality_score: -0.0052478514014865744\n",
      "Key 20151229 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.500549, quality_score: 0.0018324407119003322\n",
      "Key 20151208 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.499110, quality_score: -0.0023732776716568096\n",
      "Key 20151219 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.502878, quality_score: 0.026546008536892575\n",
      "Key 20151206 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.499448, quality_score: -0.001376198026313399\n",
      "Key 20151225 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.501842, quality_score: 0.004656983456778893\n",
      "Key 20151210 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.502175, quality_score: 0.005033056024577836\n",
      "Key 20151217 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.504815, quality_score: 0.01781564538756616\n",
      "Key 20151207 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.500265, quality_score: 0.001041269404298531\n",
      "Key 20151215 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.503588, quality_score: 0.010503247963103174\n",
      "Key 20151213 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.497793, quality_score: -0.0041918768986523615\n",
      "Key 20151209 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.499803, quality_score: -0.0005366920590455533\n",
      "Key 20151228 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.498168, quality_score: -0.003967094359346129\n",
      "Key 20151226 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.499043, quality_score: -0.005799942099344509\n",
      "Key 20151218 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.497984, quality_score: -0.006122451479250724\n",
      "Key 20151231 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.500525, quality_score: 0.002841229771042095\n",
      "Key 20151212 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.501109, quality_score: 0.003195248435718286\n",
      "Key 20151211 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.500881, quality_score: 0.0017594040762670886\n",
      "Key 20151221 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.499615, quality_score: -0.001000223091511506\n",
      "Key 20151201 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.503362, quality_score: 0.012389690959105028\n",
      "Key 20151220 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.501873, quality_score: 0.0089189370008539\n",
      "Key 20151230 | Training Loss: 1480945.125000 | Train Accuracy: 0.987793 | Test Accuracy: 0.501985, quality_score: 0.009100638269358712\n",
      "Current Layer Attributes - hidden layers:3 features count:8\n",
      "Key 20151224 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.501144, quality_score: 0.0023524462330297567\n",
      "Key 20151204 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.501790, quality_score: 0.0032979493100442816\n",
      "Key 20151216 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.505175, quality_score: 0.01172814142634493\n",
      "Key 20151222 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.504476, quality_score: 0.008398696140001648\n",
      "Key 20151214 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.500575, quality_score: 0.0011405469401928024\n",
      "Key 20151202 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.500868, quality_score: 0.0030139957764174093\n",
      "Key 20151227 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.502216, quality_score: 0.0038889584271242627\n",
      "Key 20151203 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.502572, quality_score: 0.012509228552386035\n",
      "Key 20151223 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.503254, quality_score: 0.009714014281348365\n",
      "Key 20151205 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.501580, quality_score: 0.0029535995315118496\n",
      "Key 20151229 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.502315, quality_score: 0.006230447779878231\n",
      "Key 20151208 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.502932, quality_score: 0.006103281166008262\n",
      "Key 20151219 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.502937, quality_score: 0.021947577228283954\n",
      "Key 20151206 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.499224, quality_score: -0.0015220453795839481\n",
      "Key 20151225 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.501487, quality_score: 0.0028549790492249506\n",
      "Key 20151210 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.500684, quality_score: 0.001283182770644024\n",
      "Key 20151217 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.502829, quality_score: 0.008259865026293193\n",
      "Key 20151207 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.502395, quality_score: 0.007255947815098903\n",
      "Key 20151215 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.502267, quality_score: 0.004995118909194931\n",
      "Key 20151213 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.501450, quality_score: 0.00233922140934586\n",
      "Key 20151209 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.501301, quality_score: 0.0027950836364430414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151228 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.499708, quality_score: -0.0005003936132641652\n",
      "Key 20151226 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.501686, quality_score: 0.008263779805286023\n",
      "Key 20151218 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.501341, quality_score: 0.0031312477042054762\n",
      "Key 20151231 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.501422, quality_score: 0.006236180805208447\n",
      "Key 20151212 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.500329, quality_score: 0.0007948118748140582\n",
      "Key 20151211 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.502047, quality_score: 0.0033107859678981693\n",
      "Key 20151221 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.504547, quality_score: 0.008452949056123885\n",
      "Key 20151201 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.502770, quality_score: 0.007811271535831722\n",
      "Key 20151220 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.501382, quality_score: 0.0055564365329218495\n",
      "Key 20151230 | Training Loss: 0.013566 | Train Accuracy: 0.979482 | Test Accuracy: 0.505010, quality_score: 0.017823042388496853\n",
      "Current Layer Attributes - hidden layers:3 features count:16\n",
      "Key 20151224 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.474895, quality_score: -0.01499591739758787\n",
      "Key 20151204 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.495950, quality_score: -0.002180982398504773\n",
      "Key 20151216 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.472684, quality_score: -0.018123942137858846\n",
      "Key 20151222 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.466159, quality_score: -0.018014561283632113\n",
      "Key 20151214 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.495269, quality_score: -0.002665354182122121\n",
      "Key 20151202 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.492807, quality_score: -0.0070794366326706135\n",
      "Key 20151227 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.507300, quality_score: 0.003693432585431213\n",
      "Key 20151203 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.471458, quality_score: -0.039066075202175136\n",
      "Key 20151223 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.494056, quality_score: -0.005081691651101062\n",
      "Key 20151205 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.503619, quality_score: 0.001964609863231164\n",
      "Key 20151229 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.498112, quality_score: -0.001450764584193309\n",
      "Key 20151208 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.475799, quality_score: -0.014216208718241839\n",
      "Key 20151219 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.491482, quality_score: -0.018633941728362653\n",
      "Key 20151206 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.480140, quality_score: -0.011046885707932334\n",
      "Key 20151225 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.474184, quality_score: -0.01401511796556014\n",
      "Key 20151210 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.488373, quality_score: -0.006143296777771138\n",
      "Key 20151217 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.472137, quality_score: -0.0233034726151557\n",
      "Key 20151207 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.507484, quality_score: 0.006471666103756195\n",
      "Key 20151215 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.475594, quality_score: -0.015517276651533475\n",
      "Key 20151213 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.523485, quality_score: 0.010598868985555988\n",
      "Key 20151209 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.501197, quality_score: 0.0007338137110257742\n",
      "Key 20151228 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.511832, quality_score: 0.005900407102336542\n",
      "Key 20151226 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.511450, quality_score: 0.015911015753623435\n",
      "Key 20151218 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.484042, quality_score: -0.010630423406901127\n",
      "Key 20151231 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.513905, quality_score: 0.017115393111598223\n",
      "Key 20151212 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.500292, quality_score: 0.00020108468220107777\n",
      "Key 20151211 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.498766, quality_score: -0.0005795396890901675\n",
      "Key 20151221 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.462569, quality_score: -0.01990418500094359\n",
      "Key 20151201 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.455723, quality_score: -0.03534688233590509\n",
      "Key 20151220 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.508220, quality_score: 0.009308809900895184\n",
      "Key 20151230 | Training Loss: 0.013963 | Train Accuracy: 0.754612 | Test Accuracy: 0.472850, quality_score: -0.027654057610991236\n",
      "Current Layer Attributes - hidden layers:3 features count:42\n",
      "Key 20151224 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.500556, quality_score: 0.0022474025482372914\n",
      "Key 20151204 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.499284, quality_score: -0.0024788505142464127\n",
      "Key 20151216 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.499182, quality_score: -0.003689190415624081\n",
      "Key 20151222 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.499235, quality_score: -0.002778145800301656\n",
      "Key 20151214 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.500388, quality_score: 0.0014514692365101328\n",
      "Key 20151202 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.499694, quality_score: -0.001932874424320826\n",
      "Key 20151227 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.499869, quality_score: -0.0004321493537540673\n",
      "Key 20151203 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.499965, quality_score: -0.0003197671670343954\n",
      "Key 20151223 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.499306, quality_score: -0.003941539449112562\n",
      "Key 20151205 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.500090, quality_score: 0.00032187012671320333\n",
      "Key 20151229 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.499598, quality_score: -0.002055953796290622\n",
      "Key 20151208 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.500114, quality_score: 0.0004288456325116426\n",
      "Key 20151219 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.499579, quality_score: -0.006024263507951111\n",
      "Key 20151206 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.498350, quality_score: -0.006044417176863902\n",
      "Key 20151225 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.499402, quality_score: -0.002252508510894053\n",
      "Key 20151210 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.499603, quality_score: -0.001397602836702489\n",
      "Key 20151217 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.500395, quality_score: 0.00217683867250741\n",
      "Key 20151207 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.499847, quality_score: -0.0009050682946006657\n",
      "Key 20151215 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.498884, quality_score: -0.004609425461150145\n",
      "Key 20151213 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.499883, quality_score: -0.00033900928052981913\n",
      "Key 20151209 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.501394, quality_score: 0.005550795527615799\n",
      "Key 20151228 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.499973, quality_score: -8.838084413592646e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151226 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.499950, quality_score: -0.00044481916471713587\n",
      "Key 20151218 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.498986, quality_score: -0.0044055773998393265\n",
      "Key 20151231 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.500119, quality_score: 0.0009782215419102008\n",
      "Key 20151212 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.500219, quality_score: 0.0009763810879239397\n",
      "Key 20151211 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.500444, quality_score: 0.0013775030048540043\n",
      "Key 20151221 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.499132, quality_score: -0.002957347923615317\n",
      "Key 20151201 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.500378, quality_score: 0.0020297032967128603\n",
      "Key 20151220 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.500118, quality_score: 0.0008622754679084958\n",
      "Key 20151230 | Training Loss: 0.033081 | Train Accuracy: 0.993034 | Test Accuracy: 0.499557, quality_score: -0.0030799178197634058\n"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:56:17.567730Z",
     "start_time": "2017-07-21T22:55:26.724350Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_only_vae_loss_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_only_vae_loss_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:56:17.643298Z",
     "start_time": "2017-07-21T22:56:17.569271Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "    \n",
    "    #print(cm)\n",
    "\n",
    "    label = [[\"\\n True Positive\", \"\\n False Negative \\n Type II Error\"],\n",
    "             [\"\\n False Positive \\n Type I Error\", \"\\n True Negative\"]\n",
    "            ]\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        \n",
    "        plt.text(j, i, \"{} {}\".format(cm[i, j].round(4), label[i][j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, ['Normal', 'Attack'], normalize = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:56:17.950365Z",
     "start_time": "2017-07-21T22:56:17.644909Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "past_scores = pd.read_pickle(\"dataset/scores/tf_vae_only_vae_loss_nsl_kdd_scores_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:56:17.994826Z",
     "start_time": "2017-07-21T22:56:17.951890Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>20151201</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.571151</td>\n",
       "      <td>0.056364</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20151201</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.571151</td>\n",
       "      <td>0.056364</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>20151203</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.538244</td>\n",
       "      <td>0.050518</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20151203</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.538244</td>\n",
       "      <td>0.050518</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20151217</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.541014</td>\n",
       "      <td>0.032795</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>20151217</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.541014</td>\n",
       "      <td>0.032795</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20151219</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.516278</td>\n",
       "      <td>0.032117</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>20151219</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.516278</td>\n",
       "      <td>0.032117</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>20151226</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.526129</td>\n",
       "      <td>0.031308</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>20151226</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.526129</td>\n",
       "      <td>0.031308</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>20151229</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.546709</td>\n",
       "      <td>0.030713</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>20151229</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.546709</td>\n",
       "      <td>0.030713</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>20151207</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.539209</td>\n",
       "      <td>0.029312</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>20151207</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.539209</td>\n",
       "      <td>0.029312</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>20151203</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.987793</td>\n",
       "      <td>0.504681</td>\n",
       "      <td>0.028897</td>\n",
       "      <td>30.914805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>20151203</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.987793</td>\n",
       "      <td>0.504681</td>\n",
       "      <td>0.028897</td>\n",
       "      <td>30.914805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>20151219</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.987793</td>\n",
       "      <td>0.502878</td>\n",
       "      <td>0.026546</td>\n",
       "      <td>30.914805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>20151219</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.987793</td>\n",
       "      <td>0.502878</td>\n",
       "      <td>0.026546</td>\n",
       "      <td>30.914805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>20151231</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.524822</td>\n",
       "      <td>0.026290</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>20151231</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.524822</td>\n",
       "      <td>0.026290</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>20151219</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.979482</td>\n",
       "      <td>0.502937</td>\n",
       "      <td>0.021948</td>\n",
       "      <td>33.217086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>20151219</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.979482</td>\n",
       "      <td>0.502937</td>\n",
       "      <td>0.021948</td>\n",
       "      <td>33.217086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>20151223</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.527746</td>\n",
       "      <td>0.020463</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>20151223</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.527746</td>\n",
       "      <td>0.020463</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>20151228</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.545451</td>\n",
       "      <td>0.019452</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>20151228</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.545451</td>\n",
       "      <td>0.019452</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>20151230</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.979482</td>\n",
       "      <td>0.505010</td>\n",
       "      <td>0.017823</td>\n",
       "      <td>33.217086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>20151230</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.979482</td>\n",
       "      <td>0.505010</td>\n",
       "      <td>0.017823</td>\n",
       "      <td>33.217086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>20151217</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.987793</td>\n",
       "      <td>0.504815</td>\n",
       "      <td>0.017816</td>\n",
       "      <td>30.914805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>20151217</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.987793</td>\n",
       "      <td>0.504815</td>\n",
       "      <td>0.017816</td>\n",
       "      <td>30.914805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>20151231</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.513905</td>\n",
       "      <td>0.017115</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>20151231</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.513905</td>\n",
       "      <td>0.017115</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>20151226</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.511450</td>\n",
       "      <td>0.015911</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>20151226</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.511450</td>\n",
       "      <td>0.015911</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>20151205</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.533732</td>\n",
       "      <td>0.015651</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>20151205</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.533732</td>\n",
       "      <td>0.015651</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>20151213</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.532507</td>\n",
       "      <td>0.012664</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>20151213</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.532507</td>\n",
       "      <td>0.012664</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>20151203</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.979482</td>\n",
       "      <td>0.502572</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>33.217086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>20151203</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.979482</td>\n",
       "      <td>0.502572</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>33.217086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>20151201</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.987793</td>\n",
       "      <td>0.503362</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>30.914805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>20151201</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.987793</td>\n",
       "      <td>0.503362</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>30.914805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>20151209</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.522892</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>20151209</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.522892</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>20151212</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.520431</td>\n",
       "      <td>0.012058</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>20151212</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.520431</td>\n",
       "      <td>0.012058</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>20151202</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.987793</td>\n",
       "      <td>0.502684</td>\n",
       "      <td>0.011854</td>\n",
       "      <td>30.914805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>20151202</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.987793</td>\n",
       "      <td>0.502684</td>\n",
       "      <td>0.011854</td>\n",
       "      <td>30.914805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>20151216</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.979482</td>\n",
       "      <td>0.505175</td>\n",
       "      <td>0.011728</td>\n",
       "      <td>33.217086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>20151216</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.979482</td>\n",
       "      <td>0.505175</td>\n",
       "      <td>0.011728</td>\n",
       "      <td>33.217086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>20151230</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.480834</td>\n",
       "      <td>-0.016982</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>20151230</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.480834</td>\n",
       "      <td>-0.016982</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20151214</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.467155</td>\n",
       "      <td>-0.017924</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>20151214</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.467155</td>\n",
       "      <td>-0.017924</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>20151222</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.466159</td>\n",
       "      <td>-0.018015</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>20151222</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.466159</td>\n",
       "      <td>-0.018015</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>20151216</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.472684</td>\n",
       "      <td>-0.018124</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>20151216</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.472684</td>\n",
       "      <td>-0.018124</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>20151212</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.470929</td>\n",
       "      <td>-0.018267</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20151212</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.470929</td>\n",
       "      <td>-0.018267</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>20151216</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.468256</td>\n",
       "      <td>-0.018438</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>20151216</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.468256</td>\n",
       "      <td>-0.018438</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>20151219</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.491482</td>\n",
       "      <td>-0.018634</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>20151219</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.491482</td>\n",
       "      <td>-0.018634</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20151209</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.466677</td>\n",
       "      <td>-0.019672</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>20151209</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.466677</td>\n",
       "      <td>-0.019672</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>20151221</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.462569</td>\n",
       "      <td>-0.019904</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>20151221</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.462569</td>\n",
       "      <td>-0.019904</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20151205</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.460839</td>\n",
       "      <td>-0.020163</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>20151205</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.460839</td>\n",
       "      <td>-0.020163</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>20151215</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.466848</td>\n",
       "      <td>-0.020408</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20151215</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.466848</td>\n",
       "      <td>-0.020408</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>20151221</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.457269</td>\n",
       "      <td>-0.021800</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20151221</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.457269</td>\n",
       "      <td>-0.021800</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>20151217</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.472137</td>\n",
       "      <td>-0.023303</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>20151217</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.472137</td>\n",
       "      <td>-0.023303</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.445132</td>\n",
       "      <td>-0.023735</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.445132</td>\n",
       "      <td>-0.023735</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>20151216</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.457382</td>\n",
       "      <td>-0.027599</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20151216</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.457382</td>\n",
       "      <td>-0.027599</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>20151230</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.472850</td>\n",
       "      <td>-0.027654</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>20151230</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.472850</td>\n",
       "      <td>-0.027654</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20151222</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.434051</td>\n",
       "      <td>-0.033279</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>20151222</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.434051</td>\n",
       "      <td>-0.033279</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20151229</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.450044</td>\n",
       "      <td>-0.035282</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>20151229</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.450044</td>\n",
       "      <td>-0.035282</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>20151201</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.455723</td>\n",
       "      <td>-0.035347</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>20151201</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.455723</td>\n",
       "      <td>-0.035347</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20151203</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.471458</td>\n",
       "      <td>-0.039066</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>20151203</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.471458</td>\n",
       "      <td>-0.039066</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>20151217</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.445731</td>\n",
       "      <td>-0.039359</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>20151217</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.445731</td>\n",
       "      <td>-0.039359</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>20151203</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.464632</td>\n",
       "      <td>-0.041553</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>20151203</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.464632</td>\n",
       "      <td>-0.041553</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20151231</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.459806</td>\n",
       "      <td>-0.046794</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>20151231</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.459806</td>\n",
       "      <td>-0.046794</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>20151226</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.446643</td>\n",
       "      <td>-0.067479</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20151226</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.446643</td>\n",
       "      <td>-0.067479</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>20151219</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.442603</td>\n",
       "      <td>-0.107147</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>20151219</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.442603</td>\n",
       "      <td>-0.107147</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>620 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          key  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "338  20151201               1              1     0.751854    0.571151   \n",
       "28   20151201               1              1     0.751854    0.571151   \n",
       "317  20151203               1              1     0.751854    0.538244   \n",
       "7    20151203               1              1     0.751854    0.538244   \n",
       "16   20151217               1              1     0.751854    0.541014   \n",
       "326  20151217               1              1     0.751854    0.541014   \n",
       "12   20151219               1              1     0.751854    0.516278   \n",
       "322  20151219               1              1     0.751854    0.516278   \n",
       "84   20151226               8              1     0.533915    0.526129   \n",
       "394  20151226               8              1     0.533915    0.526129   \n",
       "72   20151229               8              1     0.533915    0.546709   \n",
       "382  20151229               8              1     0.533915    0.546709   \n",
       "79   20151207               8              1     0.533915    0.539209   \n",
       "389  20151207               8              1     0.533915    0.539209   \n",
       "193  20151203               4              3     0.987793    0.504681   \n",
       "503  20151203               4              3     0.987793    0.504681   \n",
       "198  20151219               4              3     0.987793    0.502878   \n",
       "508  20151219               4              3     0.987793    0.502878   \n",
       "86   20151231               8              1     0.533915    0.524822   \n",
       "396  20151231               8              1     0.533915    0.524822   \n",
       "229  20151219               8              3     0.979482    0.502937   \n",
       "539  20151219               8              3     0.979482    0.502937   \n",
       "380  20151223               8              1     0.533915    0.527746   \n",
       "70   20151223               8              1     0.533915    0.527746   \n",
       "393  20151228               8              1     0.533915    0.545451   \n",
       "83   20151228               8              1     0.533915    0.545451   \n",
       "557  20151230               8              3     0.979482    0.505010   \n",
       "247  20151230               8              3     0.979482    0.505010   \n",
       "512  20151217               4              3     0.987793    0.504815   \n",
       "202  20151217               4              3     0.987793    0.504815   \n",
       "272  20151231              16              3     0.754612    0.513905   \n",
       "582  20151231              16              3     0.754612    0.513905   \n",
       "270  20151226              16              3     0.754612    0.511450   \n",
       "580  20151226              16              3     0.754612    0.511450   \n",
       "71   20151205               8              1     0.533915    0.533732   \n",
       "381  20151205               8              1     0.533915    0.533732   \n",
       "81   20151213               8              1     0.533915    0.532507   \n",
       "391  20151213               8              1     0.533915    0.532507   \n",
       "534  20151203               8              3     0.979482    0.502572   \n",
       "224  20151203               8              3     0.979482    0.502572   \n",
       "214  20151201               4              3     0.987793    0.503362   \n",
       "524  20151201               4              3     0.987793    0.503362   \n",
       "392  20151209               8              1     0.533915    0.522892   \n",
       "82   20151209               8              1     0.533915    0.522892   \n",
       "397  20151212               8              1     0.533915    0.520431   \n",
       "87   20151212               8              1     0.533915    0.520431   \n",
       "191  20151202               4              3     0.987793    0.502684   \n",
       "501  20151202               4              3     0.987793    0.502684   \n",
       "529  20151216               8              3     0.979482    0.505175   \n",
       "219  20151216               8              3     0.979482    0.505175   \n",
       "..        ...             ...            ...          ...         ...   \n",
       "402  20151230               8              1     0.533915    0.480834   \n",
       "92   20151230               8              1     0.533915    0.480834   \n",
       "4    20151214               1              1     0.751854    0.467155   \n",
       "314  20151214               1              1     0.751854    0.467155   \n",
       "561  20151222              16              3     0.754612    0.466159   \n",
       "251  20151222              16              3     0.754612    0.466159   \n",
       "560  20151216              16              3     0.754612    0.472684   \n",
       "250  20151216              16              3     0.754612    0.472684   \n",
       "335  20151212               1              1     0.751854    0.470929   \n",
       "25   20151212               1              1     0.751854    0.470929   \n",
       "374  20151216               8              1     0.533915    0.468256   \n",
       "64   20151216               8              1     0.533915    0.468256   \n",
       "570  20151219              16              3     0.754612    0.491482   \n",
       "260  20151219              16              3     0.754612    0.491482   \n",
       "20   20151209               1              1     0.751854    0.466677   \n",
       "330  20151209               1              1     0.751854    0.466677   \n",
       "585  20151221              16              3     0.754612    0.462569   \n",
       "275  20151221              16              3     0.754612    0.462569   \n",
       "9    20151205               1              1     0.751854    0.460839   \n",
       "319  20151205               1              1     0.751854    0.460839   \n",
       "328  20151215               1              1     0.751854    0.466848   \n",
       "18   20151215               1              1     0.751854    0.466848   \n",
       "337  20151221               1              1     0.751854    0.457269   \n",
       "27   20151221               1              1     0.751854    0.457269   \n",
       "574  20151217              16              3     0.754612    0.472137   \n",
       "264  20151217              16              3     0.754612    0.472137   \n",
       "26   20151211               1              1     0.751854    0.445132   \n",
       "336  20151211               1              1     0.751854    0.445132   \n",
       "312  20151216               1              1     0.751854    0.457382   \n",
       "2    20151216               1              1     0.751854    0.457382   \n",
       "278  20151230              16              3     0.754612    0.472850   \n",
       "588  20151230              16              3     0.754612    0.472850   \n",
       "3    20151222               1              1     0.751854    0.434051   \n",
       "313  20151222               1              1     0.751854    0.434051   \n",
       "10   20151229               1              1     0.751854    0.450044   \n",
       "320  20151229               1              1     0.751854    0.450044   \n",
       "276  20151201              16              3     0.754612    0.455723   \n",
       "586  20151201              16              3     0.754612    0.455723   \n",
       "565  20151203              16              3     0.754612    0.471458   \n",
       "255  20151203              16              3     0.754612    0.471458   \n",
       "388  20151217               8              1     0.533915    0.445731   \n",
       "78   20151217               8              1     0.533915    0.445731   \n",
       "69   20151203               8              1     0.533915    0.464632   \n",
       "379  20151203               8              1     0.533915    0.464632   \n",
       "24   20151231               1              1     0.751854    0.459806   \n",
       "334  20151231               1              1     0.751854    0.459806   \n",
       "332  20151226               1              1     0.751854    0.446643   \n",
       "22   20151226               1              1     0.751854    0.446643   \n",
       "74   20151219               8              1     0.533915    0.442603   \n",
       "384  20151219               8              1     0.533915    0.442603   \n",
       "\n",
       "     quality_score  time_taken  \n",
       "338       0.056364   27.131066  \n",
       "28        0.056364   27.131066  \n",
       "317       0.050518   27.131066  \n",
       "7         0.050518   27.131066  \n",
       "16        0.032795   27.131066  \n",
       "326       0.032795   27.131066  \n",
       "12        0.032117   27.131066  \n",
       "322       0.032117   27.131066  \n",
       "84        0.031308   23.117453  \n",
       "394       0.031308   23.117453  \n",
       "72        0.030713   23.117453  \n",
       "382       0.030713   23.117453  \n",
       "79        0.029312   23.117453  \n",
       "389       0.029312   23.117453  \n",
       "193       0.028897   30.914805  \n",
       "503       0.028897   30.914805  \n",
       "198       0.026546   30.914805  \n",
       "508       0.026546   30.914805  \n",
       "86        0.026290   23.117453  \n",
       "396       0.026290   23.117453  \n",
       "229       0.021948   33.217086  \n",
       "539       0.021948   33.217086  \n",
       "380       0.020463   23.117453  \n",
       "70        0.020463   23.117453  \n",
       "393       0.019452   23.117453  \n",
       "83        0.019452   23.117453  \n",
       "557       0.017823   33.217086  \n",
       "247       0.017823   33.217086  \n",
       "512       0.017816   30.914805  \n",
       "202       0.017816   30.914805  \n",
       "272       0.017115   32.896400  \n",
       "582       0.017115   32.896400  \n",
       "270       0.015911   32.896400  \n",
       "580       0.015911   32.896400  \n",
       "71        0.015651   23.117453  \n",
       "381       0.015651   23.117453  \n",
       "81        0.012664   23.117453  \n",
       "391       0.012664   23.117453  \n",
       "534       0.012509   33.217086  \n",
       "224       0.012509   33.217086  \n",
       "214       0.012390   30.914805  \n",
       "524       0.012390   30.914805  \n",
       "392       0.012171   23.117453  \n",
       "82        0.012171   23.117453  \n",
       "397       0.012058   23.117453  \n",
       "87        0.012058   23.117453  \n",
       "191       0.011854   30.914805  \n",
       "501       0.011854   30.914805  \n",
       "529       0.011728   33.217086  \n",
       "219       0.011728   33.217086  \n",
       "..             ...         ...  \n",
       "402      -0.016982   23.117453  \n",
       "92       -0.016982   23.117453  \n",
       "4        -0.017924   27.131066  \n",
       "314      -0.017924   27.131066  \n",
       "561      -0.018015   32.896400  \n",
       "251      -0.018015   32.896400  \n",
       "560      -0.018124   32.896400  \n",
       "250      -0.018124   32.896400  \n",
       "335      -0.018267   27.131066  \n",
       "25       -0.018267   27.131066  \n",
       "374      -0.018438   23.117453  \n",
       "64       -0.018438   23.117453  \n",
       "570      -0.018634   32.896400  \n",
       "260      -0.018634   32.896400  \n",
       "20       -0.019672   27.131066  \n",
       "330      -0.019672   27.131066  \n",
       "585      -0.019904   32.896400  \n",
       "275      -0.019904   32.896400  \n",
       "9        -0.020163   27.131066  \n",
       "319      -0.020163   27.131066  \n",
       "328      -0.020408   27.131066  \n",
       "18       -0.020408   27.131066  \n",
       "337      -0.021800   27.131066  \n",
       "27       -0.021800   27.131066  \n",
       "574      -0.023303   32.896400  \n",
       "264      -0.023303   32.896400  \n",
       "26       -0.023735   27.131066  \n",
       "336      -0.023735   27.131066  \n",
       "312      -0.027599   27.131066  \n",
       "2        -0.027599   27.131066  \n",
       "278      -0.027654   32.896400  \n",
       "588      -0.027654   32.896400  \n",
       "3        -0.033279   27.131066  \n",
       "313      -0.033279   27.131066  \n",
       "10       -0.035282   27.131066  \n",
       "320      -0.035282   27.131066  \n",
       "276      -0.035347   32.896400  \n",
       "586      -0.035347   32.896400  \n",
       "565      -0.039066   32.896400  \n",
       "255      -0.039066   32.896400  \n",
       "388      -0.039359   23.117453  \n",
       "78       -0.039359   23.117453  \n",
       "69       -0.041553   23.117453  \n",
       "379      -0.041553   23.117453  \n",
       "24       -0.046794   27.131066  \n",
       "334      -0.046794   27.131066  \n",
       "332      -0.067479   27.131066  \n",
       "22       -0.067479   27.131066  \n",
       "74       -0.107147   23.117453  \n",
       "384      -0.107147   23.117453  \n",
       "\n",
       "[620 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores.sort_values(by='quality_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:56:18.018971Z",
     "start_time": "2017-07-21T22:56:17.996279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>20151201</td>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.571151</td>\n",
       "      <td>0.056364</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>20151226</td>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.526129</td>\n",
       "      <td>0.031308</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <td>20151203</td>\n",
       "      <td>0.987793</td>\n",
       "      <td>0.504681</td>\n",
       "      <td>0.028897</td>\n",
       "      <td>30.914805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>20151219</td>\n",
       "      <td>0.979482</td>\n",
       "      <td>0.502937</td>\n",
       "      <td>0.021948</td>\n",
       "      <td>33.217086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>3</th>\n",
       "      <td>20151231</td>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.513905</td>\n",
       "      <td>0.017115</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>20151209</td>\n",
       "      <td>0.993034</td>\n",
       "      <td>0.501394</td>\n",
       "      <td>0.005551</td>\n",
       "      <td>34.947012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>20151219</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.738411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>1</th>\n",
       "      <td>20151225</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.795788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>1</th>\n",
       "      <td>20151230</td>\n",
       "      <td>0.003517</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.316731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>1</th>\n",
       "      <td>20151223</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.740419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   key  train_score  test_score  \\\n",
       "no_of_features hidden_layers                                      \n",
       "1              1              20151201     0.751854    0.571151   \n",
       "8              1              20151226     0.533915    0.526129   \n",
       "4              3              20151203     0.987793    0.504681   \n",
       "8              3              20151219     0.979482    0.502937   \n",
       "16             3              20151231     0.754612    0.513905   \n",
       "42             3              20151209     0.993034    0.501394   \n",
       "1              3              20151219     0.995758    0.500000   \n",
       "4              1              20151225     0.003690    0.500000   \n",
       "16             1              20151230     0.003517    0.500000   \n",
       "42             1              20151223     0.003690    0.500000   \n",
       "\n",
       "                              quality_score  time_taken  \n",
       "no_of_features hidden_layers                             \n",
       "1              1                   0.056364   27.131066  \n",
       "8              1                   0.031308   23.117453  \n",
       "4              3                   0.028897   30.914805  \n",
       "8              3                   0.021948   33.217086  \n",
       "16             3                   0.017115   32.896400  \n",
       "42             3                   0.005551   34.947012  \n",
       "1              3                   0.000000   30.738411  \n",
       "4              1                   0.000000   22.795788  \n",
       "16             1                   0.000000   23.316731  \n",
       "42             1                   0.000000   28.740419  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.sort_values(by='quality_score', ascending=False).groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.first().sort_values(by='quality_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:56:44.298786Z",
     "start_time": "2017-07-21T22:56:18.020348Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#key_nof_hidden '20151201_16_1'\n",
    "Train.predictions = pd.read_pickle(\"dataset/tf_vae_only_vae_loss_nsl_kdd_predictions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:56:44.323867Z",
     "start_time": "2017-07-21T22:56:44.300347Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = Train.predictions['20151228_1_1'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:56:44.344719Z",
     "start_time": "2017-07-21T22:56:44.325462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train.predictions['20151219_42_1'].loc[:,'Prediction']\n",
    "df.loc[:,'Prediction'].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:56:44.407087Z",
     "start_time": "2017-07-21T22:56:44.346452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80636831084755589"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics as me\n",
    "me.f1_score(df.loc[:,'Actual'].values.astype(int),\n",
    "            df.loc[:,'Prediction'].values.astype(int) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:56:44.422478Z",
     "start_time": "2017-07-21T22:56:44.408505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0      4159\n",
       "1.0    363423\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:56:45.190913Z",
     "start_time": "2017-07-21T22:56:44.423885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGhCAYAAADiGPptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f063812f5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:56:45.207176Z",
     "start_time": "2017-07-21T22:56:45.192543Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>0.979482</td>\n",
       "      <td>0.502055</td>\n",
       "      <td>0.005885</td>\n",
       "      <td>33.217086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <td>0.987793</td>\n",
       "      <td>0.500665</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>30.914805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.738411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>1</th>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.795788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>1</th>\n",
       "      <td>0.003517</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.316731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">42</th>\n",
       "      <th>1</th>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.740419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.993034</td>\n",
       "      <td>0.499790</td>\n",
       "      <td>-0.001025</td>\n",
       "      <td>34.947012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>0.533915</td>\n",
       "      <td>0.502968</td>\n",
       "      <td>-0.001159</td>\n",
       "      <td>23.117453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>3</th>\n",
       "      <td>0.754612</td>\n",
       "      <td>0.489737</td>\n",
       "      <td>-0.007540</td>\n",
       "      <td>32.896400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.751854</td>\n",
       "      <td>0.482744</td>\n",
       "      <td>-0.009051</td>\n",
       "      <td>27.131066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              train_score  test_score  quality_score  \\\n",
       "no_of_features hidden_layers                                           \n",
       "8              3                 0.979482    0.502055       0.005885   \n",
       "4              3                 0.987793    0.500665       0.003676   \n",
       "1              3                 0.995758    0.500000       0.000000   \n",
       "4              1                 0.003690    0.500000       0.000000   \n",
       "16             1                 0.003517    0.500000       0.000000   \n",
       "42             1                 0.003690    0.500000       0.000000   \n",
       "               3                 0.993034    0.499790      -0.001025   \n",
       "8              1                 0.533915    0.502968      -0.001159   \n",
       "16             3                 0.754612    0.489737      -0.007540   \n",
       "1              1                 0.751854    0.482744      -0.009051   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "8              3               33.217086  \n",
       "4              3               30.914805  \n",
       "1              3               30.738411  \n",
       "4              1               22.795788  \n",
       "16             1               23.316731  \n",
       "42             1               28.740419  \n",
       "               3               34.947012  \n",
       "8              1               23.117453  \n",
       "16             3               32.896400  \n",
       "1              1               27.131066  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.mean().sort_values(by='quality_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:56:45.222829Z",
     "start_time": "2017-07-21T22:56:45.208914Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.025324</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.008871</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027858</td>\n",
       "      <td>0.026485</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017167</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">42</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              train_score  test_score  quality_score  \\\n",
       "no_of_features hidden_layers                                           \n",
       "1              1                      0.0    0.030196       0.025324   \n",
       "               3                      0.0    0.000000       0.000000   \n",
       "4              1                      0.0    0.000000       0.000000   \n",
       "               3                      0.0    0.002068       0.008871   \n",
       "8              1                      0.0    0.027858       0.026485   \n",
       "               3                      0.0    0.001414       0.005041   \n",
       "16             1                      0.0    0.000000       0.000000   \n",
       "               3                      0.0    0.017167       0.013771   \n",
       "42             1                      0.0    0.000000       0.000000   \n",
       "               3                      0.0    0.000596       0.002625   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "1              1                     0.0  \n",
       "               3                     0.0  \n",
       "4              1                     0.0  \n",
       "               3                     0.0  \n",
       "8              1                     0.0  \n",
       "               3                     0.0  \n",
       "16             1                     0.0  \n",
       "               3                     0.0  \n",
       "42             1                     0.0  \n",
       "               3                     0.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T23:36:40.510518Z",
     "start_time": "2017-07-21T23:36:40.432266Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1907: RuntimeWarning: invalid value encountered in multiply\n",
      "  lower_bound = self.a * scale + loc\n",
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1908: RuntimeWarning: invalid value encountered in multiply\n",
      "  upper_bound = self.b * scale + loc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "no_of_features  hidden_layers\n",
       "1               1                    (-0.0586852729642, 0.04058373755)\n",
       "                3                                           (nan, nan)\n",
       "4               1                                           (nan, nan)\n",
       "                3                   (-0.0137100813747, 0.021061755644)\n",
       "8               1                  (-0.0530687719409, 0.0507505517522)\n",
       "                3                 (-0.00399488335604, 0.0157639380713)\n",
       "16              1                                           (nan, nan)\n",
       "                3                  (-0.0345315725298, 0.0194508685376)\n",
       "42              1                                           (nan, nan)\n",
       "                3                (-0.00617082049506, 0.00412078804357)\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fn(x):\n",
    "    #print(x)\n",
    "    return stats.norm.interval(0.95, loc=x.quality_score.mean(), scale=x.quality_score.std())\n",
    "psg.apply(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
